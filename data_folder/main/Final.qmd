---
title: "Fishy Buisness: \"Reeling in\" and Quantifying Impact of Climate Change on Groundfish Quota Prices"
subtitle: "REU Research Recap" 
date: 08/04/23
author: "Sam Turner"
format: 
  html:
    self-contained: true
    toc: true
    toc-location: left
    toc-expand: true
execute: 
  warning: false
---

# Purpose

## Goals

The goal of this document is twofold:

1.  Provide a location that I can refer back to in the F'23-S'24 school year where my work and understanding of the research topic that I began for the summer '23 REU at GMRI.
2.  Create a clean(ish) document that interested parties can read and understand what I'm researching, why it's important, and how the research process was conducted.

## Research Question

The goal of this research was to "*quantify the effects climate change has had on the quota prices of Northeast groundfish in the Gulf of Maine and surrounding marine regions*".

### Background

**Sector management** - What is it and how does it work?

In 1976 the Magnuson-Stevens Act was passed (Werner 2022) which created precedent for all fisheries in the United States to become biologically and economically sustainable. There were a variety of avenues tried in each fishery to achive this goal. The Northeast Multispecies Fishery, which governs the groundfish industry in the Gulf of Maine, Cape Cod, and George's Bank, was not satisfied with the current results of their regulations and passed Amendement 16 to their management plan in 2009 (Werner 2022). Amendment 16 created the "catch-share" program, which is also known as the quota trading system or better as the sector management system. This meant that in 2010, when the plan went into effect, groundfish fishermen were responsible for organizing themselves into groups which would become known as sectors (Werner 2022, Vasta 2019). The fishermen generally tended to organize into sectors that were differentiated by either fishing gear-type or rough geographic fishing location. The sectors also appointed sector managers (one per sector) who would oversee compliance with regulations, distribute stock quotas, as well as other responsibilities (Vasta 2019).

Stock quotas, otherwise known as Annual Catch Entitlements (ACEs), are portions of the Annual Catch Limits (ACLs) given to sectors (Vasta 2019). The ACLs are determined through biological models and surveying each year by NOAA (Vasta 2019). One of the principle functions of sector managers is distributing the ACEs to the appropriate sector members. Once NOAA has set the ACL and appropriate ACE for each sector, it has no say as to who gets what proportion of the ACE. Portions of ACE are totally determined by sector manager. However, if a sector member is unhappy with their respective quota they are allowed to trade for more. They can either buy/lease a quota or trade for a different quota. Fishermen are allowed to trade with fishermen either in their sector (known as intra-sector trading) or with fishermen outside their sector (known as inter-sector trading) (Vasta 2019). The goal of ACEs and trading was that this would promote self-regulation, efficiency and allow the market to correct any distributional errors in quota allotment (Werner 2022). There is a stark difference in ease of trading quotas within sector as opposed to between sectors. The only approval required for within secotrs is the approval from the sector manager. While between sector trades require approval from sector manager and Northeast Multispecies Fishery. Between sector trades also have a "right-of-first-refusal" where the sector manager has to check with every sector member to make sure that none of them want the quota before being able to consider trading with members of other sectors. Finally, sector managers have to be the ones looking for trades with other sectors, which they do via other sector managers (Vasta 2019). There is no centralized quota market like there is for other financial assets.

These search and bargining costs create barriers to trade. And, as we know from economic theory, barriers to trade create transaction costs which have a variety of effects (Coase 1937). Namely, inefficient allocation of goods (in this case quotas) where people who would use them cannot have access to them because of quotas trading for higher prices than they would without the barriers.

**Climate change** - Why does it matter?

Ultimately, this research project relies on the unpredictability that climate change has on groundfishing in the Gulf of Maine (GOM). We know that climate change is effecting the GOM and surrounding areas at one of the highest rates on the planet, with some estimates saying it is warming faster than 99.9% of the worlds oceans (Andy Pershing's paper from when he was at GMRI). This means that stocks and species of fish are generally moving upward as the decades pass. However, at a year-by-year level the fish are moving unpredictably. When the ocean gets unexpectadly warmer the fish can do any of the following things in the short term, move farther north, go into deeper waters, or both. Sometimes the more stubborn fish do neither. This means that at the vessel level fishermen have little to no idea where stocks are going to be moving in response to these higher temperatures (Klein et al. 2016).

Since these fish are moving more and more unpredictably as climate change worsens, we should expect this to lead to more and more distributional errors by NOAA and the ACE allotment. We should expect these sectors which are organize on rough geographic location to begin to trade quotas at an increased frequency since in the short term they cannot relocate to the new regions the fish are moving to and trading the quotas will help recooperate some cost of operations. In essance, we should expect to see a heavier reliance on the quota trading system due to climate change. And, as we discussed above, the more reliance that is placed on the inter-sector trading system (which this is) should lead to higher quota prices due to the transaction costs from the barriers to trade.

### Relevance

Lee and Demarest (2023) made their conclusions of market efficacy without considering climate change, it is quite possible that some of their conclusions are incorrect. In fact, we have seen biologists, ecologists, and many other scientists highlight the danger of omitting climate change from models (Kerr et al. 2022). Economic models are no less prone to this error of conclusions from climate change omission than any other discipline. Thus, I expected to find that some of the conclusions that Lee and Demarest (2023) find with quota market efficiency are flawed.

The sector management system is a very strange, complex, and confusing system and understanding its efficiency is important. If it is not operating as we would expect it to, this would mean that either the fishermen or the consumers are losing out on producer or consumer surplus respectively. There has also been a history of fisheries in the Gulf of Maine collapsing, such as the [shrimp](https://www.bangordailynews.com/2013/12/03/news/regulators-shut-down-gulf-of-maine-shrimp-fishery-say-stock-has-collapsed/) and [herring](https://www.nationalfisherman.com/northeast/feds-declare-east-coast-herring-fishery-a-disasterhttps://www.nationalfisherman.com/northeast/feds-declare-east-coast-herring-fishery-a-disaster) fisheries, so it is far-fetched to think the groundfish fishery could collapse. Preserving the market for all parties is of the utmost importance. And a vital part of maintaining the market is ensuring that it is working well, which is where the Lee and Demarest (2023) analysis comes in.

# Results

## Data

The data used was gathered from two sources. The first is the quota and assorted market data used in the Lee and Demarest (2023) paper. Thankfully, they were gracious enough to make (most of) their data public on [GitHub](https://github.com/NEFSC/READ-SSB-Lee-QuotaPriceReplication). Although, there is almost no documentation for exploring the repository. Luckily, the README for my [GOM-REU](https://github.com/Sturner21/GOM-REU) repository has a navigational section and outlines most of the documents that you'd find helpful.

The second source of data is an R package from NOAA and is called `ecodata`. This has a plethora of semi-clean data sets, some of which are climate change related. I used a couple different data sets in there which are cleaned below.

## Clean Data

### Clean main Data

The first step to replicating and then expanding on Lee and Demarest's research was making their data usable. Below is the code that was done to prepare their data for examination.

```{r}
#Load the haven library so that the stata (.dta) data files can be read
library(tidyverse)
library(haven)

#Datasets needed should already be in current directory and can be directly accessed with this
quarterly_fish_prices <- read_dta("quarterly_ols_coefs_from_R_2022_03_04.dta")
Tspatial_lags <- read_dta("Tspatial_lags_2022_03_04.dta")
```

```{r}
#Only select the variables of interest from each data set

#Select variables from quarterly fish prices
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  dplyr::select(fishing_year, q_fy, b, dateq, stockcode, stock_id, stock, nespp3, stockarea, spstock2, quota_remaining_BOQ, fraction_remaining_BOQ, proportion_observed, live_priceGDP)

#Select variables of interest from spatial lags
Tspatial_lags <-  Tspatial_lags %>% 
  dplyr::select(fishing_year, dateq, stockcode, WTswt_quota_remaining_BOQ, WTDswt_quota_remaining_BOQ)
```

```{r}
#Background: there were a different number of observations between the two data sets. The following code corrects for the observations present in "quarterly_fish_prices" which were not present in Tspatial_lags

#Further select down for the variables that are shared between quarterly and Tspatial data sets

#Want to get this to the 672 observations of "Tspatial_lags"
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  #Helps get rid of some observations - Gets to 884 rows
  dplyr::filter(stockcode != 1818 & stockcode != 9999) %>% 
  #Gets down to 680 rows
  dplyr::filter(fishing_year >= 2010 & fishing_year <= 2019)
```

```{r}
replic <-  dplyr::right_join(Tspatial_lags, quarterly_fish_prices, by = c("fishing_year", "dateq", "stockcode")) %>% 
  #This filters out the remaining non-overlapping parts of our dataset
  dplyr::filter(!is.na(WTswt_quota_remaining_BOQ))
```

```{r}
#Add in factor variable quarter - Technically is already present, but in the sake of laziness so I can preserve code of regressions I will keep this bit in here. It affects final result in no way, simply makes my life a little easier transfering data from "Cragg.qmd" to here

replic <-  replic %>%
  #Rearrange 'replic' so that we have each stock in chronological order
  dplyr::arrange(stockcode) %>% 
  #Add in a quarter variable
  dplyr::mutate(quarter=rep(c("Q1","Q2","Q3","Q4"), times=168), .after = fishing_year) %>% 
  #Make the quarter variable a factor variable so regression recognizes it as a dummy variable
  dplyr::mutate(quarter = as.factor(quarter))
```

```{r}
#Lee & D say on page 8 that any quota price that is either negative or NA was replaced with a 0 in their analysis. So, the following code does the same in my data set

replic <- replic %>%
  #Stage 1 is to make any negative values into 0's
  dplyr::mutate(b = case_when(b < 0 ~ 0,
                              b >=0 ~ b)) %>% 
  #Stage 2 is to make any NA's into 0's
  dplyr::mutate(b = replace_na(b, 0))
```

### Clean and incorporate ecodata

```{r}
#| eval: false

#Installing ecodata is different than other R packages - Below is the tools you'll need if you're installing it for the first time
library(devtools)
remotes::install_github("noaa-edab/ecodata", build_vignettes=TRUE)

#Then just load it like any other library
library(ecodata)
```

```{r}
#The above code chunk won't eval so just use this one to load in ecodata if it's already installed on your system

library(ecodata)
```

```{r}
#Below are the data sets of interest for this research

#For bottom sea temperature
#Data is only annually so need to fix
bottom_temp <- ecodata::bottom_temp

#Harmful algae blooms
#Data is only annually so need to fix
habs <- ecodata::habs

#Heatwave data
#Data is only annuall so need to fix
heatwave <- ecodata::heatwave

#Sea surface temerpature anomoly - not as great as bottom temp anomoly but still good
sst <- ecodata::seasonal_oisst_anom

#Storminess
#Data is only annually so need to fix
storms <- ecodata::storminess
```

Some notes from "Cragg.qmd" (the intermediary and more messy version of this) about the following cell which organizes the ecodata into a more usable format:

For `bottom_temp`: Also, the GOM and GB (George's Bank) observations are different. Thus, when merging with the `replic` data set, we will have to account for that difference. Shouldn't be too hard though - could always use a mutate(EPU=case_when()) and merge on the EPU.

For `heatwave`: Tracks both the intensity and duration of both the Sea-Surface-Temp (SST) and Bottom_Surface_Temp (BST). The variables `duration-BottomDetrended` and `duration-SurfaceDetrended` should describe the duration of heatwaves in a year. Are they summed for the year though?

The detrended in the variable names corresponds to the authors taking away the trend of global warming from the data. Meaning any deviation from 0 in the data corresponds with extra heating or cooling that can be attributed to region specific climate change. For more information refer to the link at the bottom of the help page for the `heatwave` data set.

For `sst`: The link to the documentation can be seen [here](https://noaa-edab.github.io/tech-doc/seasonal-sst-anomalies.html). Describes the exact coding of "spring", "summer", "fall", and "winter". And, I recode into quarters to make my life easier since they have exact overlap.

```{r}
#This cell reformats the ecodata data sets into more usable formats

#Widen the bottom_temp data set and resrtic for only observations we care about
bottom_temp <- bottom_temp %>% 
  #Restrict to only the time frame in question
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Exclude MAB and SS
  filter(EPU != "MAB" & EPU != "SS") %>% 
  #Pivot the table wider - take observations from the same year and make them into columns instead of redundant rows to make mergering easier later
  pivot_wider(names_from = Var, values_from = Value)

habs <- habs %>% 
  #Restrict area to only GOM in aggregate
  filter(Var == "Gulf_of_Maine_All") %>% 
  #Only include years we care about
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Rename the Value to the variable it's observing
  rename(Algae = Value) %>% 
  #Select only the columns needed for merging
  dplyr::select(!c(Source, Var))

heatwave <- heatwave %>% 
  #Restict area to George's Bank (GB) and Gulf of Maine (GOM)
  filter(EPU == "GB" | EPU == "GOM") %>% 
  #Restrict time frame to 2010-2019
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Keep maximum intensity and duration of SS and BS heatwaves
  filter(Var != "cumulative intensity-SurfaceDetrended" & Var != "cumulative intensity-BottomDetrended") %>% 
  #Pivot this wider so that we only have 1 row for each observation in both GB and GOM
  #Deselect Units because it fucks with the pivot wider function
  dplyr::select(!Units) %>% 
  pivot_wider(names_from = Var, values_from = Value)

#SST is an anomoly measure, similar to the heatwave data set - Seems redundant due to the heatwave dataset. Heatwave seems much more complete and has more robust methods
sst <- sst %>% 
  #Select years 2010-2019
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Recode the time series in terms of quarters instead of seasons to make merge easier later
  mutate(q = case_when(Var == "Winter" ~ "Q1",
                       Var == "Spring" ~ "Q2",
                       Var == "Summer" ~ "Q3",
                       Var == "Fall" ~ "Q4"),
         .after = Var) %>% 
  #Drop the Var colum since it's unnecessary
  dplyr::select(!Var) %>% 
  #Filter for only area of interest
  filter(EPU != "MAB")

#Probably need to come back and make this wider, but this works for now
storms <- storms %>% 
  #Select only years we care about
  filter(Year >= 2010 & Year <= 2019) %>% 
  #Consider only areas of interest
  filter(EPU == "GOM" | EPU == "GB") %>% 
  #Group by the year and general region to get a better sense of cumulative storms for this region
  group_by(Year, Var) %>% 
  summarize(mean_events = mean(Value),
            untis = units,
            EPU = EPU) %>% 
  #This data represents days in the year that a "storm" was recorded from a wind and wave      perspective
  #Make the data set tidy where each observation (year) is a row
  pivot_wider(names_from = Var, values_from = mean_events) %>% 
  #Take the average of the GOM observations and just use the GB for their respective rows
  mutate(Gale_Wind = case_when(EPU == "GOM" ~ mean(c(`Eastern Gulf of Maine_GaleWind`, `Western Gulf of Maine_GaleWind`), na.rm = T),
                               EPU == "GB" ~ `Georges Bank_GaleWind`), .after = EPU) %>% 
  #Same process as above mutate but for WaveHeight instead
  mutate(Wave_Height = case_when(EPU == "GOM" ~ mean(c(`Eastern Gulf of Maine_WaveHeight`, `Western Gulf of Maine_WaveHeight`), na.rm = T),
                               EPU == "GB" ~ `Georges Bank_WaveHeight`), .after = Gale_Wind) %>% 
  #Select only the columnds that matter for merging
  dplyr::select(Year, EPU, Gale_Wind, Wave_Height) %>% 
  #Rename the Year column to Time to make merge easier
  rename(Time = Year)
```

```{r}
#Rewrite the EPU's of replic to be less specified so we can merge data sets
replic <- replic %>% 
  mutate(EPU = case_when(stockarea == "CCGOM" ~ "GOM",
                         stockarea == "GBE" ~ "GB",
                         stockarea == "GBW" ~ "GB",
                         stockarea == "GB" ~ "GB",
                         stockarea == "GOM" ~ "GOM",
                         #Maddy says that Plaice is normally caught in GB
                         stockarea == "Unit" ~ "GB",
                         #SNEMA is closer to GOM than GB so we count it as such
                         stockarea == "SNEMA" ~ "GOM"), .after = stockcode)
```

```{r}
#Join the heat related data sets together (heatwave and bottom_temp)
temp1 <- bottom_temp %>%
  #Join the heatwave and bottom temp data sets together
  right_join(heatwave, by = join_by(Time, EPU)) %>%
  #Select the columns that will be most helpful
  dplyr::select(Time, EPU, `bottom temp anomaly in situ`, `sst anomaly in situ`, `duration-SurfaceDetrended`, `duration-BottomDetrended`) %>% 
  #Join again with storms data
  right_join(storms, by = join_by(Time, EPU)) %>% 
  #Join again with algae bloom data - Don't have data on GB so might need to come back and alter that for regression
  left_join(habs, by = join_by(Time, EPU)) %>% 
  #Repeat each thing for 4 quarters
  slice(rep(1:n(), each = 4)) %>% 
  mutate(q = rep(c("Q1","Q2","Q3","Q4"), times=20), .after = Time) %>% 
  #Rename q to quarter and turn into factor
  rename(quarter = q) %>% 
  mutate(quarter = as.factor(quarter))
```

```{r}
#This chunk appears to join everything together correctly

#Create the merged price and climate data set called "replic2"
replic2 <- replic %>% 
  right_join(temp1, join_by("fishing_year"=="Time", EPU, quarter))
```

There were two suggestions from Kanae about how to better improve the model. The first was to consider when we saw comparatively extreme Sea Surface Temperatures (SST's) and Bottom Surface Temperatures (BST's). The most obvious way of doing this was to create a dummy variable in R with a cut-off. We arbitrarily settled on the 90th percentile for this but it could easily have been any other number. In the future, consider using log-standard deviations since we see the quota prices are approaching a normal shape when taking a log scale. The goal of adding the 90th percentile of SST and BST anomaly was to see if extreme heatwaves are reflected in quota prices. We would expect to see that they are since the would drive stocks either deeper or farther north. In either case it would be away from where the fishermen were/are fishing. The second suggestion was to consider whether a stock had been retroactively had its stock assessment changed. This is slightly complicated to explain since it requires summarizing [this paper](https://watermark.silverchair.com/fsac140.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA1YwggNSBgkqhkiG9w0BBwagggNDMIIDPwIBADCCAzgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMuDCAQBUvFVDDE5hmAgEQgIIDCbvKvXe1ZJlS7Q6bODE2nplq6uFxvy6xnpz_56f3tsJQigWarQnZWHeP3zFHeX12dA2-Bla8udV1gzt_kRf9ywTpq6sxyGuduY05EhOfLS4PKqCn5EQGNWtghLdVBvjyPGf_L6-raF_KFn49VQsWkNdJ9tU6tC1OrEimuYm9JaZ0QJ3-e1rtv1p_kWXIeJ3qmYN09aqHAkoXCPmqBtHe-EKsHRXJTnDgNJm_trhLw0np7WDolNBx1s56PMZaafqFxXEn_T8J4CckCztvFBWvu-za4C6ykfVoiSyIjCsSyXUP6SBOHdUn5sgxBhDUWrL3L1rB6lQw14vf8qE7ZW3SL3oej5xJpZJw4Xbla5czMEzBJmDQcSo7FD00JgN3l_LZbMopQ8PmGnH2ulKabLFdG1--BH6hhBo1nG_76DDg-dJMSBaBzqeHhkw48oa9aIBuw5i5YGOt4TFoYOQfLAbXo_XLrWiiJbbBKo29wxSpxrgwmRGhpGPMe-Qcv_xKax697bBY91e3AHFXA7_zomgnMc0BDipoOznYPjWU6nDGJ5yjT5NiQRaicVr6WhYA4RI2OCHJGehoJ_9qZlZlrRs2t2Ce8HL7qfGlR_zh01jnzJjQgfgwDm-c0naYvZKkVuV4xGLKt8FEPoF5HIBCPgYOsdZErUMkAV8zCmQDdLuqEVlI0-gl0a6il61Rgy-aC47-YHwfLv2RhGIJrZqWsc9MnXamWHSbVOJte7Y1fm-5HqAQnA2PFljnwi2MTtH3XxqP1tmwF5Q82Vi8Mo1D1Dg45Fkj51e46W878swa_SRFgy6jHQHrrjbHBT-iSFFs-9-c3VsbLF9DN9oxs2cor41WTyC0TWGOjBWFxSzT07SGILFF3oFKlc39WQDSGc4KvqiapN1K0kfJOUwO0ODDIpDAP1csUxf-hp9OdvgTduW9KQRuhdRUCNi84yqN8FleE0VcAdyzdVRBang5KOiFQs2fWshzsNftzNsL_E_uSkYxvhzT1WtiCy5RcaQgUIW5-L65zYwTHpNvLwDKGQ) (authored by some GMRI staff) but basically the amount of fish that can be taken from the ocean collectively, known as the Total Allowable Catch (TAC), is determined before the fishing year by suggestions from biologists. In reality it's much more complicated but for our sake that's all we need to know. So, biologists attempt to estimate the spawning stock biomass (SSB) to make an informed decision about the populations ability to reproduce and thus determine how much of it can be taken out. However, (more than) a couple of years we have seen some estimates of the SSB for different stocks so bad that researchers have had to go back and recalculate how many there must have been given where they see the population now. In [this table](https://academic.oup.com/view-large/377923733) from the Kerr et al. (2022) we have documentation of which stocks had to have their populations reevaluated because of over-fishing. Thus, I used this table to add a dummy variable to my data set on whether the stock required an adjustment that year. The goal of the sock adjustment variable was to test whether modeling errors were affect the market of quota prices.

```{r}
#Add in the 90th percentile dummy variable for SST and BST

replic2 <- replic2 %>% 
  #Create the ifelse command to test if the observation makes the 90th percentile cutoff
  mutate(SST90 = ifelse(`sst anomaly in situ` >= quantile(replic2$`sst anomaly in situ`, probs = .9), 1, 0),.after = `sst anomaly in situ`) %>% 
  #Repeat but for BST
  mutate(BST90 = ifelse(`bottom temp anomaly in situ` >= quantile(replic2$`bottom temp anomaly in situ`, probs = .9), 1, 0),.after = `bottom temp anomaly in situ`)
```

```{r}
#Add in dummy variable for SSB getting revised after-the-fact because previous estimations were realized to be incorrect
#Dummy "stock_adjustment": 0 = no adjustment for stock in given year, 1 = adjustment made to stock in given year

replic3 <- replic2 %>% 
  mutate(stock_adjustment = case_when(
    #George's Bank Cod
    stock_id == "CODGBE" & fishing_year == 2012 ~ 1,
    stock_id == "CODGBW" & fishing_year == 2012 ~ 1,
    stock_id == "CODGBE" & fishing_year == 2013 ~ 1,
    stock_id == "CODGBW" & fishing_year == 2013 ~ 1,
    
    #Gulf of Maine Cod
    stock_id == "CODGMSS" & fishing_year == 2019 ~ 1,
    
    #George's Bank Haddock
    stock_id == "HADGBE" & fishing_year == 2015 ~ 1,
    stock_id == "HADGBW" & fishing_year == 2015 ~ 1,
    stock_id == "HADGBE" & fishing_year == 2017 ~ 1,
    stock_id == "HADGBW" & fishing_year == 2017 ~ 1,
    stock_id == "HADGBE" & fishing_year == 2019 ~ 1,
    stock_id == "HADGBW" & fishing_year == 2019 ~ 1,
    
    #Gulf of Maine Haddock
    stock_id == "HADGM" & fishing_year == 2019 ~ 1,
    
    #George's Bank Yellowtail Flounder
    stock_id == "YELGB" & fishing_year == 2011 ~ 1,
    stock_id == "YELGB" & fishing_year == 2012 ~ 1,
    stock_id == "YELGB" & fishing_year == 2013 ~ 1,
    
    #Cape Cod and Gulf of Maine Yellowtail Flounder
    stock_id == "YELCCGM" & fishing_year == 2012 ~ 1,
    stock_id == "YELCCGM" & fishing_year == 2015 ~ 1,
    stock_id == "YELCCGM" & fishing_year == 2017 ~ 1,
    stock_id == "YELCCGM" & fishing_year == 2019 ~ 1,
    
    #George's Bank Winter Flounder
    stock_id == "FLWGB" & fishing_year == 2015 ~ 1,
    stock_id == "FLWGB" & fishing_year == 2017 ~ 1,
    stock_id == "FLWGB" & fishing_year == 2019 ~ 1,
    stock_id == "FLWGB" & fishing_year == 2020 ~ 1,
    
    #Witch Flounder
    stock_id == "WITGMMA" & fishing_year == 2015 ~ 1,
    
    #American Plaice
    stock_id == "PLAGMMA" & fishing_year == 2008 ~ 1,
    stock_id == "PLAGMMA" & fishing_year == 2012 ~ 1,
    stock_id == "PLAGMMA" & fishing_year == 2015 ~ 1,
    stock_id == "PLAGMMA" & fishing_year == 2017 ~ 1,
    stock_id == "PLAGMMA" & fishing_year == 2019 ~ 1,
    
    #Pollock
    stock_id == "POKGMASS" & fishing_year == 2015 ~ 1,
    stock_id == "POKGMASS" & fishing_year == 2017 ~ 1,
    stock_id == "POKGMASS" & fishing_year == 2019 ~ 1,
    
    #White Hake
    stock_id == "HKWGMMA" & fishing_year == 2017 ~ 1,
    stock_id == "HKWGMMA" & fishing_year == 2019 ~ 1,
    
    #Set the default value of the column to 0 for all the stocks which didn't get SSB revised
    .default = 0
  ), .after = EPU)
```

## Comment on Distribution

One of the things highlighted in [this](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-lognormal-model-with-fancy-multilevel-things) EXTREMELY helpful article on implementing hurdle models, we want the distribution of our dependent variable to be as close to normal as possible. Currently, the distribution looks like this:

```{r}
ggplot(data = replic)+
  aes(x=b)+
  geom_histogram()+
  labs(title="Quota Price Distribution",
       subtitle="After Replacing with 0's",
       x="Quota Price (2009 $)",
       y="Observations")

#ggsave("QuotaPriceDist.png", dpi = 320)
```

Needless to say, this is not normal or anything approaching it at first glance. However, if we change the scale of the x-axis to the natural logarithm (log with base *e*), then the story begins to change.

```{r}
ggplot(data = replic)+
  aes(x=log(b))+
  geom_histogram()+
  labs(title="Quota Price Distribution on Natural Log Scale",
       subtitle="After Replacing with 0's",
       x="Quota Price ($, Ln Scale)",
       y="Observations")
```

Not perfect, but certainly MUCH more normal than we had before. As we will see soon, this shift which enhances normality leads to the exponential model fitting better than the normal one. If you're curious why, I would refer back to [this article](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-lognormal-model-with-fancy-multilevel-things) again. Andrew Wheiss does a much better job at explaining how it works than I do. It's in the section outlining 0 counts with the GDP per capita and its distribution.

## Model - No Climate Change

We first want to create a baseline as close to Lee and Demarest (2023) as possible. If we were to jump right into modeling with climate change, we would have no idea if the modeling algorithms which differ between R and Stata are responsible for the different findings, or if there is something larger at play. Thus, the model summary shown below is the attempt at replicating Table 5 (on page 12) of Lee and Demarest (2023).

```{r}
library(modelsummary)
library(mhurdle)
```

```{r}
#Create the linear and expoential models

#This hurdle model is the linear one, which comes from the 'dist="n"' arguement
hurdle.1a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, 
                 data = replic,
                 h2 = TRUE, dist = "n", method = "bhhh") 

#This hurdle model is the expoential one, which comes from the 'dist="ln"' arguement
hurdle.1b <- update(hurdle.1a, dist = "ln")
```

```{r}
#Display the hurdle models

modelsummary(models = list("Exponential" = hurdle.1b, "Linear" = hurdle.1a),
             #Set the stars to signify statistical significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Make the standard errors go away
             statistic = NULL,
             #Rename the coefficients to make the table presentable
             coef_map = c(
                             "h1.quota_remaining_BOQ" = "Quota Remaining [H1]",
                             "h1.fraction_remaining_BOQ" = "Fraction Quota Remaining [H1]",
                             "h1.proportion_observed" = "Fraction of Catch Observed [H1]",
                             "h1.quarterQ2" = "Q2 [H1]",
                             "h1.quarterQ3" = "Q3 [H1]",
                             "h1.quarterQ4" = "Q4 [H1]",
                             "h1.(Intercept)" = "Intercept [H1]",
                             "h2.live_priceGDP" = "Live Price [H2]",
                             "h2.quota_remaining_BOQ" = "Quota Remaining [H2]",
                             "h2.fraction_remaining_BOQ" = "Fraction Quota Remaining [H2]",
                             "h2.proportion_observed" = "Fraction of Catch Observed [H2]",
                             "h2.WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining [H2]",
                             "h2.WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining [H2]",
                             "h2.quarterQ2" = "Q2 [H2]",
                             "h2.quarterQ3" = "Q3 [H2]",
                             "h2.quarterQ4" = "Q4 [H2]",
                             "h2.(Intercept)" = "Intercept [H2]"),
             coef_omit = "sd.sd|pos",
             gof_omit = "dpar",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "nobs.zero", "clean" = "N [0]", "fmt" = 0),
               list("raw" = "nobs.pos", "clean" = "N [Count]", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "R2.zero", "clean" = "R² [0]", "fmt" = 3),
               list("raw" = "R2.pos", "clean" = "R² [Count]", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

This is an excellent time to explain how to interpret this type of a hurdle model in context:

This hurdle model is basically two models. The first model is a logistic regression and its independent variables are denoted with the `[H1]` tag in the model summary. This logistic regression is taking the independent variables and predicting whether the dependent variable, in this case the quota price, is either 0 or positive (in which case it assigns a value of 1 to the quota). The coefficients in the first hurdle describe whether the variable is associated with an increased or decreased probability the quota will trade at a price depending on the sign of the coefficient. If observation passes the first "hurdle" (in our case is non-zero) then it keeps all data associated with it and runs an OLS regression to find what variables directly influence the price of the quotas. (As a side note, this ability to use different variables in the different hurdles is what separates Cragg's hurdle model from a Tobit model). This modeling technique allows us to reliably predict the correct amount of 0's in the dependent distribution without creating a truncated non-zero distribution. The OLS coefficients from the second hurdle can be interpreted just as you would with any other OLS regression. Again, [this article](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-lognormal-model-with-fancy-multilevel-things) by Andrew Weiss is very helpful in understanding why people use a hurdle model. The goodness of fit interpretations are fairly straightforward. Before discussing what has been printed in the model summary it is important to discuss what has not been printed. A variable called `dpar`. I have no idea what this thing is other than it has something to do with the "hessian" which is a matrix filled with partial derivatives related to the models. I can't understand it when looking it up online since I haven't taken Linear Algebra yet, but if you were to just render this model summary it would appear. With that out of the way, understanding the other goodness of fit statistics is easy. `N`, `N [0]`, and `N [Count]` are all simply the number of observations in our data set. While `N [0]` is the number of observations we have where the quota price is equal to 0 and `N [Count]` is the number of observations we have where the quota price is not equal to 0. The `R2 [0]` is the $R^2$ value for the logistic regression. It has the 0 in the name because it is commonly known as the 0 regression since it the part of the model which describes the 0's. The `R2 [Count]` is the $R^2$ value for the OLS regression. It is called count because it describes the data which has a count or a positive amount. I am unsure if these are pure, adjusted, or pseudo $R^2$'s as the documentation is very unclear and often you have to dive down deep rabbit holes to find out what them mean. For example, the description of Log Likelihood is hidden in another package and the authors of `mhurdle` only leave a one line comment telling you to go looking there for it. In any case, this brings us finally to the Log Likelihood value. This describes the whole model (both `H1` and `H2`) and should be interpreted as any other log likelihood would be, only in comparison of fit between similar models and never absolutely.

```{r}
#Also important is the OLS regression

OLS_replic <- lm(data = replic, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter)
```

```{r}
#Display the OLS replication
modelsummary(models = list("OLS" = OLS_replic),
             #Set the stars to signify statistical significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Remove the standard errors
             statistic = NULL,
             #Rename the coefficients to make the table presentable
             coef_rename = c("(Intercept)" = "Intercept",
                             "live_priceGDP" = "Live Price",
                             "quota_remaining_BOQ" = "Quota Remaining",
                             "proportion_observed" = "Fraction of Catch Observed",
                             "WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining",
                             "WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining",
                             "quarterQ2" = "Q2",
                             "quarterQ3" = "Q3",
                             "quarterQ4" = "Q4"),
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "r.squared", "clean" = "R²", "fmt" = 3),
               list("raw" = "adj.r.squared", "clean" = "Adj. R²", "fmt" = 3),
               list("raw" = "aic", "clean" = "AIC", "fmt" = 3),
               list("raw" = "bic", "clean" = "BIC ", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

This model output should need no explanation, since it is a standard OLS regression.

One interesting control to consider that Lee and Demarest (2023) left out is a time series control. They kind of controlled for this with by including the quarters but including the year should also capture market trends and expectations which are endogenous(?) to the model.

```{r}
#Simply add fishing year into the equation eith the same exact code as above
hurdle.1aa <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + fishing_year, 
                 data = replic,
                 h2 = TRUE, dist = "n", method = "bhhh") 

hurdle.1ab <- update(hurdle.1aa, dist = "ln")
```

```{r}
#Display the replicated Linear and Expoential models but with a time series control
modelsummary(models = list("Linear Robust Time Series" = hurdle.1aa, "Exponential Robust Time Series" = hurdle.1ab),
             #Set the stars to signify statistical significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Remove the standard errors
             statistic = NULL,
             #Rename the coefficients to make the table presentable
             coef_rename = c("h1.(Intercept)" = "Intercept [H1]",
                             "h1.quota_remaining_BOQ" = "Quota Remaining [H1]",
                             "h1.fraction_remaining_BOQ" = "Fraction Quota Remaining [H1]",
                             "h1.proportion_observed" = "Fraction of Catch Observed [H1]",
                             "h1.quarterQ2" = "Q2 [H1]",
                             "h1.quarterQ3" = "Q3 [H1]",
                             "h1.quarterQ4" = "Q4 [H1]",
                             "h2.(Intercept)" = "Intercept [H2]",
                             "h2.live_priceGDP" = "Live Price",
                             "h2.quota_remaining_BOQ" = "Quota Remaining [H2]",
                             "h2.fraction_remaining_BOQ" = "Fraction Quota Remaining [H2]",
                             "h2.proportion_observed" = "Fraction of Catch Observed [H2]",
                             "h2.WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining",
                             "h2.WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining",
                             "h2.quarterQ2" = "Q2 [H2]",
                             "h2.quarterQ3" = "Q3 [H2]",
                             "h2.quarterQ4" = "Q4 [H2]",
                             "h2.fishing_year" = "Year"),
             coef_omit = "sd.sd|pos",
             gof_omit = "dpar",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "nobs.zero", "clean" = "N [0]", "fmt" = 0),
               list("raw" = "nobs.pos", "clean" = "N [Count]", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "R2.zero", "clean" = "R² [0]", "fmt" = 3),
               list("raw" = "R2.pos", "clean" = "R² [Count]", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

Including the year does not seem to make a large difference in the hurdle model's fit. This can quickly be determined by the almost identical log likelihood values between the time series and non-time series models. In addition, the $R^2$ values of both the count and 0 models are also nearly identical between the time series and non-time series models. The only significant difference is between the linear and exponential models themselves, since the exponential fits the count data much better than the linear one does which predicting the 0's virtually just as well.

```{r}
#Simply add fishing year into the OLS regression. It could have a difference here
OLS_replic_time_series <- lm(data = replic, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + fishing_year)
```

```{r}
#Display the time series replication of the OLS regression
modelsummary(models = list("OLS" = OLS_replic_time_series),
             #Set the stars to signify statistical significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Remove standard errors
             statistic = NULL,
             #Rename the coefficients to make the table presentable
             coef_rename = c("(Intercept)" = "Intercept",
                             "live_priceGDP" = "Live Price",
                             "quota_remaining_BOQ" = "Quota Remaining",
                             "proportion_observed" = "Fraction of Catch Observed",
                             "WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining",
                             "WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining",
                             "quarterQ2" = "Q2",
                             "quarterQ3" = "Q3",
                             "quarterQ4" = "Q4",
                             "fishing_year" = "Year"),
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "r.squared", "clean" = "R²", "fmt" = 3),
               list("raw" = "adj.r.squared", "clean" = "Adj. R²", "fmt" = 3),
               list("raw" = "aic", "clean" = "AIC", "fmt" = 3),
               list("raw" = "bic", "clean" = "BIC ", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

Again, not a huge difference when adding in the time series control.

Compare with the image of Table 5 (pictured below) taken from Lee and Demarest (2023) and we can see that our models stack up well with theirs. They are not perfect replications, however I think we can chalk that up to our modeling software being different. I chose R and they chose Stata which have different maximization algorithms.

![](images/Table%205.png)

We can of course, also make these replication models even more complicated. We can consider whether the error terms of both parts of the hurdle are correlated and set something called the `finalHessian` to `TRUE` in `mhurdle`'s arguments. I do not understand the hessian currently because of my lack of linear algebra. Below is an example of considering if the errors of the two parts of the model are correlated.

```{r}
hurdle.2a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, data = replic,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh", finalHessian = TRUE)

hurdle.2b <- update(hurdle.2a, start = coef(hurdle.2a), robust = FALSE)
```

```{r}
modelsummary(models = list("Exponential Robust" = hurdle.2a, "Exponential Non-Robust" = hurdle.2b),
             #Set the stars to signify statistical significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Remove standard errors
             statistic = NULL,
             #Rename the coefficients to make the table presentable
             coef_rename = c("h1.(Intercept)" = "Intercept [H1]",
                             "h1.quota_remaining_BOQ" = "Quota Remaining [H1]",
                             "h1.fraction_remaining_BOQ" = "Fraction Quota Remaining [H1]",
                             "h1.proportion_observed" = "Fraction of Catch Observed [H1]",
                             "h1.quarterQ2" = "Q2 [H1]",
                             "h1.quarterQ3" = "Q3 [H1]",
                             "h1.quarterQ4" = "Q4 [H1]",
                             "h2.(Intercept)" = "Intercept [H2]",
                             "h2.live_priceGDP" = "Live Price",
                             "h2.quota_remaining_BOQ" = "Quota Remaining [H2]",
                             "h2.fraction_remaining_BOQ" = "Fraction Quota Remaining [H2]",
                             "h2.proportion_observed" = "Fraction of Catch Observed [H2]",
                             "h2.WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining",
                             "h2.WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining",
                             "h2.quarterQ2" = "Q2 [H2]",
                             "h2.quarterQ3" = "Q3 [H2]",
                             "h2.quarterQ4" = "Q4 [H2]",
                             "corr12" = "Correlation of Errors [H1 & H2]"),
             coef_omit = "sd.sd|pos",
             gof_omit = "dpar",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "nobs.zero", "clean" = "N [0]", "fmt" = 0),
               list("raw" = "nobs.pos", "clean" = "N [Count]", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "R2.zero", "clean" = "R² [0]", "fmt" = 3),
               list("raw" = "R2.pos", "clean" = "R² [Count]", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

Both models considered use an exponential distribution of the errors since we've seen this be a better fit for the count data. Their difference is in whether they use robust (heteroskedastic) or non-robust (homeoskedastic) assumptions about the errors while considering if the errors are correlated. However, as one can easily see the models are not only (almost) identical, but also have the same significance on their Correlation of Errors term. Meaning twofold: robust and non-robust assumption doesn't matter for this case and there is not significant evidence to suggest that the error terms of the models are correlated. Which I believe means that we can interpret this as model exogeneity.

## Model - Climate Change

To begin, we should keep the same structure of models as the above section to see how including climate change has impacted the model. However, in the above section we have considered several different ways to look at the hurdle model such as robustness, correlation, etc. Therefore, we should also look at those models for this comparison's sake. Especially considering that we will see the above models become fairly non-robust when we start adding climate data.

```{r}
#Create the models which include climate change variables
#Call the "churdles" for "climate hurdle"

#This model is exponential non-correlated robust hurdle model
churdle.4a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Wave_Height + BST90 + stock_adjustment | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + Wave_Height + BST90 + stock_adjustment, 
                 data = replic3,
                 h2 = TRUE, dist = "ln", method = "bhhh")

#This model creates a exponential correlated robust hurdle model
churdle.5a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Wave_Height + BST90 + stock_adjustment | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + Wave_Height + BST90 + stock_adjustment, data = replic3,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh", finalHessian = TRUE)

#This model updates churdle.5a to be non-robust
churdle.5b <- update(churdle.5a, start = coef(churdle.5a), robust = FALSE)

#This model updats churdle.5a to be linear
churdle.5c <- update(churdle.5a, dist = "n")
```

```{r}
#This summarizes the models created in the previous cell

modelsummary(models = list("Linear Robust Correlated Climate" = churdle.5c, "Exponential Robust Correlated Climate" = churdle.5a, "Exponential Non-Robust Correlated Climate" = churdle.5b, "Exponential Robust Non-Correlated Climate" = churdle.4a),
             #Set the stars to correct significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Prevents standard error from being displayed since I think there's too much going on with it
             statistic = NULL,
             coef_rename = c("h1.(Intercept)" = "Intercept [H1]",
                             "h1.quota_remaining_BOQ" = "Quota Remaining [H1]",
                             "h1.fraction_remaining_BOQ" = "Fraction Quota Remaining [H1]",
                             "h1.proportion_observed" = "Fraction of Catch Observed [H1]",
                             "h1.quarterQ2" = "Q2 [H1]",
                             "h1.quarterQ3" = "Q3 [H1]",
                             "h1.quarterQ4" = "Q4 [H1]",
                             "h1.`bottom temp anomaly in situ`" = "BST Heatwave Maximum [H1]",
                             "h1.`sst anomaly in situ`" = "SST Heatwave Maximum [H1]",
                             "h1.`duration-BottomDetrended`" = "BST Heatwave Duration [H1]",
                             "h1.`duration-SurfaceDetrended`" = "SST Heatwave Duration [H1]",
                             "h1.Wave_Height" = "Storm Days (Waves) [H1]",
                             "h1.BST90" = "BST in 90th Percentile [H1]",
                             "h1.stock_adjustment" = "Stock Assessment Adjusted Post-Facto [H1]",
                             "h2.(Intercept)" = "Intercept [H2]",
                             "h2.live_priceGDP" = "Live Price [H2]",
                             "h2.quota_remaining_BOQ" = "Quota Remaining [H2]",
                             "h2.fraction_remaining_BOQ" = "Fraction Quota Remaining [H2]",
                             "h2.proportion_observed" = "Fraction of Catch Observed [H2]",
                             "h2.WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining [H2]",
                             "h2.WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining [H2]",
                             "h2.quarterQ2" = "Q2 [H2]",
                             "h2.quarterQ3" = "Q3 [H2]",
                             "h2.quarterQ4" = "Q4 [H2]",
                             "h2.`bottom temp anomaly in situ`" = "BST Heatwave Maximum [H2]",
                             "h2.`sst anomaly in situ`" = "SST Heatwave Maximum [H2]",
                             "h2.`duration-BottomDetrended`" = "BST Heatwave Duration [H2]",
                             "h2.Wave_Height" = "Storm Days (Waves) [H2]",
                             "h2.BST90" = "BST in 90th Percentile [H2]",
                             "h2.stock_adjustment" = "Stock Assessment Adjusted Post-Facto [H2]",
                             "corr12" = "Correlation of Errors [H1 & H2]"),
             coef_omit = "sd.sd|pos",
             #gof_omit = "dpar",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "nobs.zero", "clean" = "N [0]", "fmt" = 0),
               list("raw" = "nobs.pos", "clean" = "N [Count]", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "R2.zero", "clean" = "R² [0]", "fmt" = 3),
               list("raw" = "R2.pos", "clean" = "R² [Count]", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

This is really hard to read because of how long it is. Let's cut this down to only the variables which are significant across models.

```{r}
#Same as above cell but leave out non-statistically-significant results


modelsummary(models = list("Linear Robust Correlated Climate" = churdle.5c, "Exponential Robust Correlated Climate" = churdle.5a, "Exponential Non-Robust Correlated Climate" = churdle.5b, "Exponential Robust Non-Correlated Climate" = churdle.4a),
             #Set the stars to correct significance level
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Prevents standard error from being displayed since I think there's too much going on with it
             statistic = NULL,
             
             coef_map = c("h1.(Intercept)" = "Intercept [H1]",
                             "h1.quota_remaining_BOQ" = "Quota Remaining [H1]",
                             "h1.fraction_remaining_BOQ" = "Fraction Quota Remaining [H1]",
                             "h1.proportion_observed" = "Fraction of Catch Observed [H1]",
                             "h1.quarterQ2" = "Q2 [H1]",
                             "h1.quarterQ3" = "Q3 [H1]",
                             "h1.quarterQ4" = "Q4 [H1]",
                             "h1.`bottom temp anomaly in situ`" = "BST Heatwave Maximum [H1]",
                             "h1.`duration-SurfaceDetrended`" = "SST Heatwave Duration [H1]",
                             "h1.Wave_Height" = "Storm Days (Waves) [H1]",
                             "h2.(Intercept)" = "Intercept [H2]",
                             "h2.live_priceGDP" = "Live Price [H2]",
                             "h2.quota_remaining_BOQ" = "Quota Remaining [H2]",
                             "h2.fraction_remaining_BOQ" = "Fraction Quota Remaining [H2]",
                             "h2.proportion_observed" = "Fraction of Catch Observed [H2]",
                             "h2.WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining [H2]",
                             "h2.WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining [H2]",
                             "h2.quarterQ3" = "Q3 [H2]",
                             "h2.quarterQ4" = "Q4 [H2]",
                             "h2.`bottom temp anomaly in situ`" = "BST Heatwave Maximum [H2]",
                             "h2.`sst anomaly in situ`" = "SST Heatwave Maximum [H2]",
                             "h2.BST90" = "BST in 90th Percentile [H2]"),
             coef_omit = "sd.sd|pos",
             #gof_omit = "dpar",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "nobs.zero", "clean" = "N [0]", "fmt" = 0),
               list("raw" = "nobs.pos", "clean" = "N [Count]", "fmt" = 0),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "R2.zero", "clean" = "R² [0]", "fmt" = 3),
               list("raw" = "R2.pos", "clean" = "R² [Count]", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

We can see a couple of different things from these models. In general, almost all of the models first hurdle (the logistic regression) predict 0's at about the same level of competence. All $R^2$'s float between 0.373-0.375. However, we see some decent variation in the $R^2$'s for the count regression (the second hurdle - OLS regression). The Linear Robust Correlated model does the worst job with a count $R^2$ of 0.293. While the Exponential Robust Non-Correlated model does the best job with a count $R^2$ of 0.532. While the other two models have a count $R^2$ of 0.444. Another area where these fits are echoed are in the Log Likelihoods. Since all models have the same $R^2$'s for the 0 count data then we can interpret the differences in their Log Likelihoods as due to the better or worse fit from the $R^2$ of the count data.

Arguably the most interesting finding from including the climate change, playing with the robustness, and the correlation is that we see (sometimes high) levels of varying significance on the model's coefficients which Lee and Demarest (2023) suggested would be almost staple in any variation of their model. I at least assume that they would think their findings would be consistent given the language they use describing their statistical significance as well as the conclusions they drew on market efficiency from them. I will not include the intercepts in this discussion since they give us almost no interesting findings. For the first model, we see that Fraction of Quota Remaining and all four quarters are the only explanatory variables which remain significant across all models. This leaves Quota Remaining and Fraction of Catch Observed as only sometimes significant. It makes sense that we wouldn't see significance on the Fraction of Catch Observed since we don't even see significance on it in Lee and Demarest (2023). However, it is odd that we don't see the same effect being picked up by both the Quota Remaining and Fraction of Quota Remaining since they are, in effect, measuring the same thing. Which is whether having quota leftover and not traded affects the quota prices. Further consideration as to why we see this is needed. Especially since we see that Quota Remaining is very significant in Lee and Demarest (2023). They don't describe what the cut-offs are for the stars in their paper, but if they're anything like the levels of significance present in R then it means they are significant to a p-value of almost 0.

Another interesting finding is that many of the climate coefficients are not significant. Bottom Surface temperature is the only one, which kind-of makes sense since we are considering groundfish species which (like the name implies) live near the bottom of the ocean. Thus, it would make sense that bottom temperature would affect them while sea surface temperature may not. However, it is a little odd that the duration of a heatwave does not seem to affect the fish as much as the maximum temperature would, at least as it relates to quota prices. One of the things that Lee and Demarest (2023) harped on in their introduction is that the switch to an ITQ-like system of fishing allows for fishermen to pick the days they want to go out and reduces effort in a good way. We can see this in this model since there is no change in probability of quota price depending on the number of days that in a year which had storms. This shows that fishermen are picking their days to go out on the water and aren't dangerously rushing into the water like they would have when effort was regulated instead of quantity. Interestingly we again don't see an impact on quota prices from the BST 90th percentile dummy variable. Again, I don't have an explanation for why this could be occurring. My best current guess would be that 90th percentile isn't a high enough cut-off for us to see the significance. Considering that BST maximum already only barely reaches significance then it wouldn't be a stretch to say that we might need to get to the 95th or 99th percentile before we start seeing the fish really starting to change their behavior in ways that would dupe fishermen who have been in the area and doing this for decades. Then we also see that the stock adjustment has no affect on the quota prices of the quota. This seems rather improbable and when I discussed this with Kanae and she agreed. It would be great if we could implement a lag variable to measure this on a stock-by-stock basis since I think that would allow us to capture the affect of having to change the stock measurements with respect to scarcity. However, each species life cycle is different and therefore the lag factor would have to change with the species. That's too much work for this summer and probably when I get back to school too. Especially considering what Amanda (Hart\[?\]) said about non-Cod/Haddock stocks. (She said they were very understudied and it's hard to get any information on them). For the second model, we see that Live Price, Quota Remaining, and Fraction of Catch Observed are the only variables which are statistically significant across all models. Both Live Price and Quota Remaining make sense as to why they would be important in determining the quota price. However, to see that Fraction of Catch Observed doesn't impact the probability of a quota price being 0 or positive, but then is highly significant to the price of the quota is a head-scratcher and borders on paradoxical. The only interpretation that I'm left with is that only certain species are monitored heavily and those species definitely have a higher price and were never zero which is why it's in the count but not in the zero. From background knowledge I know that there has been a new an increased effort by NOAA with the [help of GMRI to install cameras](https://gmri.org/projects/electronic-monitoring-em/) in an effort to increase compliance in respecting legal sizes of fish through an increase in monitoring effort. It could be that only species whose quotas sell for more (never 0) have experienced this increase in monitoring effort coincidentally. Further investigation is certainly warranted and would be a good place to resume in the school year. Continuing with the trend from the first hurdle we see that the climate variables are not very good at describing the changes we see in quota prices of the fish. Some of the variables are significant in some of the models but there are no variables significant across all models. Yet again pointing to the interesting notion that climate change is not being reflected well in our models.

This lack of representation of climate change influence in quota price models could mean a few things. The first, and most probably, is that we don't have enough data. Whether that be not far enough forward or backwards since our data only goes from 2010-2019. It's possible that we either haven't seen a threshold reached that will come in the future where we see species really start to leave their historic grounds to get away from warming waters or it's possible that we have seen that threshold but the data available doesn't reach far enough back for us to capture that trend. The second, and less probable of the two given what the current literature of both biology and economics would suggest, is that climate change is not having an impact on these stocks in a way that has been reflected in the quota prices.

```{r}
#Time Fixed Effect model
OLS_time_fixed <- lm(data = replic3, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + `bottom temp anomaly in situ` + BST90 + `sst anomaly in situ` + + `duration-BottomDetrended` + `duration-SurfaceDetrended`+ Gale_Wind + Wave_Height + stock_adjustment + quarter + fishing_year) 

#Time and Stock Fixed Effect Model
OLS_full_fixed <- lm(data = replic3, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + `bottom temp anomaly in situ` + BST90 + `sst anomaly in situ` + + `duration-BottomDetrended` + `duration-SurfaceDetrended`+ Gale_Wind + Wave_Height + stock_adjustment + quarter + fishing_year + stock_id) 
```

```{r}
modelsummary(models = list("Time Fixed Effect" = OLS_time_fixed, "Stock and Time Fixed Effect" = OLS_full_fixed),
             #The "˙" character is from pressing "option+H"
             stars = c('^' = 0.1, '*' = .05, '**' = .01, '***' = 0.001),
             #Get rid of the standard errors in the output because it's too many numbers with these huge models
             statistic = NULL,
             #The renaming of the coefficients is crazy out of order. Just a heads up for when I come back to this
             coef_rename = c("(Intercept)" = "Intercept",
                             "live_priceGDP" = "Live Price",
                             "quota_remaining_BOQ" = "Quota Remaining",
                             "fraction_remaining_BOQ" = "Fraction Quota Remaining",
                             "proportion_observed" = "Fraction of Catch Observed",
                             "WTswt_quota_remaining_BOQ" = "Distance Lag of Quota Remaining",
                             "WTDswt_quota_remaining_BOQ" = "Inverse Distance Lag of Quota Remaining",
                             "quarterQ2" = "Q2",
                             "quarterQ3" = "Q3",
                             "quarterQ4" = "Q4",
                             "bottom temp anomaly in situ" = "BST Heatwave Maximum",
                             "sst anomaly in situ" = "SST Heatwave Maximum",
                             "duration-BottomDetrended" = "BST Heatwave Duration",
                             "duration-SurfaceDetrended" = "SST Heatwave Duration",
                             "Gale_Wind" = "Storm Days (Wind)",
                             "Wave_Height" = "Storm Days (Waves)",
                             "BST90" = "BST in 90th Percentile",
                             "stock_adjustment" = "Stock Assessment Adjusted Post-Facto",
                             "fishing_year" = "Year"),
             coef_omit = "stock_id",
             gof_map = list(
               list("raw" = "nobs", "clean" = "N", "fmt" = 0),
               list("raw" = "r.squared", "clean" = "R²", "fmt" = 3),
               #This funky "²" can be accessed through the emoji keyboard on mac with
               #"control+command+space" and searching for super-script
               list("raw" = "adj.r.squared", "clean" = "Adj. R²", "fmt" = 3),
               list("raw" = "aic", "clean" = "AIC", "fmt" = 3),
               list("raw" = "bic", "clean" = "BIC ", "fmt" = 3),
               list("raw" = "logLik", "clean" = "Log Likelihood", "fmt" = 3)
             ))
```

Conversely to the hurdle models, we see climate change being screamed in the OLS models. This is quite odd to have such differences present.

# Conclusions

Overall, we find that there is evidence that climate change is being reflected in the quota prices. There is strong evidence from both theory and the fixed effects models. While weak evidence is present in the hurdle models. However, as Kane pointed out, if we rerun the hurdle models with less explanatory variables we will probably start to see them reflecting different levels of significance. With the amount of explanatory variables we have we are starting to push the hurdle models to their effective limits.

Since climate change is reflected in the quota prices, this should mean (according to Lee and Demarest) that the quota market could benefit from some changes to trading. Namely, removing barriers to trade should help reduce the quota prices since it will allow the quotas to go more freely to the fishermen who want/need them.

# Directions For Further Research

One of the most obvious places for improvement in this research is considering the amount of trades that take place during a quarter as a control. In my climate change models I assume some processes using economic theory that may not be taking place. I assume that climate change is causing an increase in quota trades because we see stock populations shifting unpredictably. Then, from economic theory we assume due to the high transaction costs from some (arguably poor) interesting market design that this will lead to higher quota prices. However, I never formally tested whether whether we actually see more trades occurring from climate change. This would be rather easy to test and an excellent opportunity to use some causal inference techniques since we will need to test against the counter-factual that the market is just maturing. This will probably lend itself well to a diff-in-diff model but we can cross that bridge when we get there.

In the back of my mind I should be considering whether the stock is a choke species. Probably these choke species are the ones who are being traded more often. Creating a cut-off and then coding a dummy variable for designated choke species is similar to the BST90 and SST90 variables for creating a significant variable.

Another area of improvement would be in understanding why, in the climate hurdle models, do we see differences in significance in the same variables across the different hurdles? Such as Fraction of Catch Observed; it is significant in the first hurdle but not the second hurdle model which is very difficult to interpret. Diving into and finding why this is would be very interesting and is something that should be answered before SPIRES in S'24.
