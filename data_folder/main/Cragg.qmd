---
title: "Cragg"
format: 
  html:
    self-contained: true
    toc: true
    toc-location: right
execute: 
  warning: false
---

## Libraries

There are tons of warnings from packages masking and such. So, in the YAML I have suppressed warnings for rendering.

```{r}
#Load in most of the necessary packages for this project
library(tidyverse)
library(mhurdle)
#These just keep overwriting eachother so I'll load them in as I need them in this document
#library(AER)
#library(pscl)
#library(countreg)
#library(lmtest)
```

Why would we want to use Cragg's double-hurdle model?

We observe a lot of 0's in this data, so if we used an OLS regression then we would not get an accurate picture as to what the true determinants are for the quota prices. A Tobit model is also a double-hurdle model, which allows for the researcher to determine first whether the data is going to be a zero, and then if it's not, what value it would take on. The downside to this method is that the Tobit assumes that the same process and variables determine both whether the data will be a 0 or what number it will be. Cragg recognized that there were important use cases in economics for this, such as consumer spending. However, he also realized that the variables which determine consumer spending are not the same across buying the good. Cragg's model allows for different variables to be included in both stages of the Tobit model to better account for different preferences acting in varying degrees in the consumer's purchasing process.

# Replicate Documentation

The documentation for the *mhurdle* package can be found [here](https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf). We will be replicating Section 5, **Software Rationale**. In this replication we will be trying to fit the hurdle model to how much households spend on the "fees and admission" good. This should have a lot of 0 observations because some households will not spend any money on them at all, others will infrequently, and others still will spend a good portion of their income on it. This should be a good introduction package, and this data should represent the quota prices that I ultimately want to test well.

```{r}
#Grab the data that the documentation uses
data("Interview", package = "mhurdle")
```

The covariates of this data set are:

1.  Income - Annual net income by consumption unit
2.  SMSA - Does the household live in SMSA (yes or no)
3.  Age - The age of the reference person of the household
4.  Educ - The number of years of education of the reference person of the household
5.  Sex - The sex of the reference person of the household (male or female)
6.  Size - The number of persons in the household
7.  Month - The month of the interview (Between 1 to 12)

The analysis uses the households spending on the "fees and admission" good which is called *shows* in the data set.

```{r}
#What is the mean value, how many observations are 0, and what is the average for non-0 observations
round(c(mean = mean(Interview$shows),
        "% of 0" = mean(Interview$shows == 0),
        "mean pos" = mean(Interview$shows) / mean(Interview$shows > 0)),
      2)
```

These are good summary statistics, but if you actually want to see some of the data from *Interview*, here:

```{r}
#Just check out some of the data set
head(Interview %>% 
  #select() is masked by some other pakcage so I have to explicitly call it from dplyr to use   #it
  dplyr::select(income, smsa, age, educ, sex, size, month, shows))
```

To illustrate the use of *mhurdle* we will start with a single equation tobit model using both the normal and log-normal specification

```{r}
#Not sure this ever appears again. Certainly not in the below table so not sure what the point of it is
N010I <- mhurdle(shows ~ 0 | linc + smsa + age + educ + size, 
                 data = Interview,
                 h2 = TRUE, dist = "n", method = "bhhh") 

L010I <- update(N010I, dist = "ln")
```

The naming convention for the models in this documentation is as follows:

-   Each model is 5 digits long

-   The first capital letter denotes the distribution used - N for normal and L for log-normal

-   The second, third, and fourth are binary representing if the first, second, or third hurdle has been used (1 if used, 0 if not used)

-   The fifth digit is either D or I depending on if the model is dependent (D) or independent (I)

We should consider two other hurdle models, namely selection process and infrequency of purchase.

```{r}
#Creates a lot of models, some of which are displayed in the table below
L100D <- mhurdle(shows ~ smsa + age + educ + size | linc, data = Interview,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh",
finalHessian = TRUE)
L100D2 <- update(L100D, start = coef(L100D), robust = FALSE)
L110D <- update(L100D, h2 = TRUE)
L110D2 <- update(L110D, start = coef(L110D), robust = FALSE)


L001D <- mhurdle(shows ~ 0 | linc | smsa + age +
educ + size, data = Interview,
                   h2 = FALSE, corr = TRUE, method = "bhhh",
                 finalHessian = TRUE)
L001D2 <- update(L001D, start = coef(L001D), robust = FALSE) 
L011D <- update(L001D, h2 = TRUE)
L011D2 <- update(L011D, start = coef(L011D), robust = FALSE)
```

```{r}
#Makes some more of the models that appear in the table below
L101D <- mhurdle(shows ~ educ + size | linc |
smsa + age, data = Interview,
                    h2 = FALSE, method = "bhhh", corr = TRUE,
                 finalHessian = TRUE)
L101D2 <- update(L101D, start = coef(L101D), robust = FALSE) 
L111D <- update(L101D, h2 = TRUE)
L111D2 <- update(L111D, start = coef(L111D), robust = FALSE) 
L111I <- update(L111D, corr = FALSE)
L111I2 <- update(L111I, start = coef(L111I), robust = FALSE)
```

The results are shown in the table below

```{r}
#Sets up the list of the models that are considered in the table below. Plus, it outlines how to seperate the table by the different models which will be helpful in my final rendering
models <- list(L110D = L110D, L011D = L011D, 
               L101D = L101D, L111D = L111D, L111I = L111I)
coefs <- unique(Reduce("c", lapply(models, function(x) names(coef(x)))))
coefs1 <- grep("h1", coefs)
coefs2 <- grep("h2", coefs)
coefs3 <- grep("h3", coefs)
coefso <- (1:length(coefs))[-c (coefs1, coefs2, coefs3)]
custcoefs <- coefs
custcoefs[coefso] <- c("$\\sigma$", "$\\rho_{12}$", "$\\alpha$", "$\\rho_{23}$", "$\\rho_{13}$")
coefs.h <- grep("h.\\.", coefs)
#custcoefs[coefs.h] <- substring(custcoefs[coefs.h], 4, 1E4)
groups <- list("**Hurdle 1**" = 1:length(coefs1),
               "**Hurlde 2**" = (length(coefs1)+1):(length(coefs1) + length(coefs2)),
               "**Hurdle 3**"= (length(coefs1)+length(coefs2)+1):(length(coefs1) + length(coefs2) + length(coefs3)),
               "**Others**" = (length(coefs1) + length(coefs2) + length(coefs3) + 1) : (length(coefs1) + length(coefs2) + length(coefs3) + length(coefso)))
#coeflabels <- gsub("h[1-3]\\.(\\w*)", "\\1", coefs)
```

```{r}
#| eval: true
#| results: asis

#In the original document this is a texreg because it's a PDF. However, I am rendering a HTML document so I've made the slight alteration of using the htmlreg command

texreg::htmlreg(models, reorder.coef = c(coefs1, coefs2, coefs3, coefso), 
       custom.coef.names = custcoefs,#[c(coefs1, coefs2, coefs3, coefso)],
       caption = "Estimation of cencored models for the fees and admissions good",
       label = "tab:estimations",
       groups = groups
       )
```

For my work, it looks like I'll want to use the code that creates the the model **L110D** since that creates the two part hurdle.

Below are some significance tests that the paper runs. I do not fully understand this so circling back may be helpful. However, to start I just want to replicate Lee & Demarest's findings so all these tests aren't necessary. Only once I start considering my own will they be helpful.

```{r}
library(lmtest)
lrtest(L111D, L111I)
```

```{r}
vuongtest(L111D, L111I, type = "nested")
```

This Vuong Test examines the probability that the two models considered fit the data equally as likely. The null hypothesis is that they fit equally well, the alternative is that one fits better than the other. It is currently unclear which model fits the data better with this output, but regardless we see a quite large p-value of 0.233 indicating that the null hypothesis is nowhere close to being rejected. Meaning that we can be quite confident that these two models fit the data just as well as each other.

```{r}
ndvuongtest(L110D, L011D)
```

The negative sign on the `z` value of `ndvuongtest()` would indicate that the p-tobit model (L110D) fits the data better than the three-hurdle/ general selection model (L011D). Additionally, a `z` value this large creates a p-value which is statistically significant. Indicating that there is a significant difference between how the models fit the data. Paired with the parsimony principle (should look this up because I have no idea what it means) this means that we should be picking model L110D to represent our data if we could only choose between L110D and L011D.

This concludes the replication of the documentation.

# Clean main Data

We need to get the data from "quarterly_ols_coefs_from_R_2022_03_04.dta", "spatial_lags_2022_03_04.dta", and "Tspatial_lags_2022_03_04.dta" into a singular, tidy, data frame so that we can start replicating the results. Below is the code of how I combined and cleaned the data.

```{r}
#We need to load all the different data sets in
library(haven)

quarterly_fish_prices <- read_dta("quarterly_ols_coefs_from_R_2022_03_04.dta")
Tspatial_lags <- read_dta("Tspatial_lags_2022_03_04.dta")
```

```{r}
#Select the variables of interest from each data set

#Select variables from quarterly fish prices
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  dplyr::select(fishing_year, q_fy, b, dateq, stockcode, stock_id, stock, nespp3, stockarea, spstock2, quota_remaining_BOQ, fraction_remaining_BOQ, proportion_observed, live_priceGDP)

#Select variables of interest from each data set
Tspatial_lags <-  Tspatial_lags %>% 
  dplyr::select(fishing_year, dateq, stockcode, WTswt_quota_remaining_BOQ, WTDswt_quota_remaining_BOQ)
```

There are a different amount of observations in the two data sets, so lets make them the same length so that the join is 1:1.

```{r}
head(Tspatial_lags)
```

```{r}
#Further select down for the variables that are shared between quarterly and Tspatial data sets

#Want to get this to the 672 observations of "Tspatial_lags"
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  #Helps get rid of some observations - Gets to 884 rows
  dplyr::filter(stockcode != 1818 & stockcode != 9999) %>% 
  #Gets down to 680 rows
  dplyr::filter(fishing_year >= 2010 & fishing_year <= 2019)
```

Let's try to combine the data sets now and see which ones have missing values to pinpoint the non-overlapping observations.

```{r}
replic <-  dplyr::right_join(Tspatial_lags, quarterly_fish_prices, by = c("fishing_year", "dateq", "stockcode")) %>% 
  #This filters out the non-overlapping parts of our dataset
  dplyr::filter(!is.na(WTswt_quota_remaining_BOQ))

#Even though there are som missing values for "b", there shouldn't be an issue running regressions on it
```

May not need this below chunk. Technically, there is quarter already coded into the data set, but I'm not sure how much I trust it.

```{r}
#Add in factor variable quarter

replic <-  replic %>%
  #Rearrange 'replic' so that we have each stock in chronological order
  dplyr::arrange(stockcode) %>% 
  #Add in a quarter variable
  dplyr::mutate(quarter=rep(c("Q1","Q2","Q3","Q4"), times=168), .after = fishing_year) %>% 
  #Make the quarter variable a factor variable so regression recognizes it as a dummy variable
  dplyr::mutate(quarter = as.factor(quarter))
```

```{r}
head(replic)
```

Finally, we need to adapt the quota prices to the assumptions made by Lee & Demarest, we need to make all the NA's and negative values into 0's.

```{r}
replic <- replic %>%
  #Stage 1 is to make any negative values into 0's
  dplyr::mutate(b = case_when(b < 0 ~ 0,
                              b >=0 ~ b)) %>% 
  #Stage 2 is to make any NA's into 0's
  dplyr::mutate(b = replace_na(b, 0))
```

Let's check the distriubtion to make sure that we got it right.

```{r}
ggplot(data = replic)+
  aes(x=b)+
  geom_histogram()+
  labs(title="Quota Price Distribution",
       subtitle="After Replacing with 0's",
       x="Quota Price ($)",
       y="Observations")
```

It's also important to see what it looks like on a log scale because of the implications this will have on our modeling with the `mhurdle` package later.

```{r}
ggplot(data = replic)+
  aes(x=log(b))+
  geom_histogram()+
  #scale_x_continuous(trans = "log10")+
  labs(title="Quota Price Distribution on Natural Log Scale",
       subtitle="After Replacing with 0's",
       x="Quota Price ($, Ln Scale)",
       y="Observations")
```

This distribution is not normal, but it is certainly more normal shaped than the one above. This explains why later we find the log-normal models to have a better fit.

In fact, Lee and Demarest already recognized that this exponential distribution fit the data better. Their linear hurdle model would be the equivalent of `dist = "n"` for the model using a normal distribution. Their expoential hurdle model uses the natural log scale `dist = "ln"`. Thus, we can fully replicate Lee and Demarest's findings and the only changes present should be the maximization method that the software uses. For this article, we arbitrarily chose BHHH since that's what is in the documentation and we have not experimented with other algorithms.

Most of the discoveries made here, as well as those made in the subsections of the Hurlde from the With Climate Change section, were thanks to [this article](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-lognormal-model-with-fancy-multilevel-things). This professor explores what the coefficients mean in a hurdle model as well as exploring what to do when the guide (he made mostly for himself) doesn't work. This is an excellent resource and one that should be read carefully when returning to this project in the F'23-S'24 school year.

# Model - No Climate Change

## mhurdle package method

```{r}
#For quick referenceing of models because "summary()" doesn't work with mhurdle package
library(modelsummary)
```

Let's try to make a hurdle model.

```{r}
hurdle.1a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, 
                 data = replic,
                 h2 = TRUE, dist = "n", method = "bhhh") 

hurdle.1b <- update(hurdle.1a, dist = "ln")
```

```{r}
modelsummary(models = list("1a" = hurdle.1a, "1b" = hurdle.1b),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

This verbatim makes the models that we started with in the documentation for the hurdle model. The model summary lets us easily take a look at them and their significance.

As Kanae says, even a monkey can make a regression but can you understand its output? Currently, no. However, using the link to [this paper](https://journals.sagepub.com/doi/pdf/10.1177/1536867X0900900405) I should be able to parse together what the outputs are for this model. Below is my best attempt at decoding what I've found.

Turns out this page was not helpful in this regard at all. I did like the article though so I'm keeping it here.

Let's try to make some more complicated ones.

```{r}
hurdle.2a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, data = replic,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh", finalHessian = TRUE)

hurdle.2b <- update(hurdle.2a, start = coef(hurdle.2a), robust = FALSE)
hurdle.2c <- update(hurdle.2a, h2 = TRUE)
hurdle.2d <- update(hurdle.2c, start = coef(hurdle.2c), robust = FALSE)
```

```{r}
modelsummary(models = list("2a" = hurdle.2a, "2b" = hurdle.2b, "2c" = hurdle.2c, "2d"= hurdle.2d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

The more complicated models have now been replicated. Let's try the most complex finally and see what they look like.

```{r}
hurdle.3a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, data = replic,
                     h2=FALSE, corr=TRUE, method="bhhh", finalHessian=TRUE)
hurdle.3b <- update(hurdle.3a, start = coef(hurdle.3a), robust = FALSE)
hurdle.3c <- update(hurdle.3a, h2=TRUE)
hurdle.3d <- update(hurdle.3c, start = coef(hurdle.3c), robust = FALSE)
```

```{r}
modelsummary(models = list("3a" = hurdle.3a, "3b" = hurdle.3b, "3c" = hurdle.3c, "3d"= hurdle.3d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

## UVA method

I will need to use different packages than *countreg* since I need distributions similar to Poisson but that are continuous. Internet says the *GAMLSS* has such distributions.

```{r}
library(AER)
library(pscl)
library(countreg)
```

```{r}
mod1 <- glm(data = replic, family = "gaussian",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) 

mod1 %>% 
  summary()
```

Quota prices that are negative are probably just modelling error since econ theory (and common sense) would suggest that agents wouldn't pay to have quota taken off their hands. So, for the sake of argument, let's eliminate them from consideration so we can try and fit a poisson distribution.

```{r}
#| warning: false

#Suppres the ungodly amount of warnings from the errors of trying the poisson

glm(data = replic, family = "poisson",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) %>% 
  summary()
```

```{r}
#Use the quasipoisson to correct for the prices which are not integers but it also means that I have no AIC
#Not the worst thing in the world since I had AIC=inf when using just "poisson"
mod2 <- glm(data = replic, family = "quasipoisson",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) 

mod2 %>% 
  summary()
```

Hurdle model

Could just multiply by 100000 and round

Can I just make a two stage model - part one being logistic and part two being OLS (or something similar)

```{r}
#| eval: false
hurdle(data = replic, b ~ .) %>% 
  summary()
```

## OLS method

Trying to replicate the OLS model.

```{r}
#Seeing if we can replicate the 'OLS' column of Table 5 from Lee & Demarest

lm(data = replic, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter) %>% 
  summary()
```

Should try with logging everything to adjust for non-linearities. Can't hurt...

Should include dummy variable for the stock codes.

Also, should include an interaction term between some of the stock codes and the climate change indicators.

```{r}
#| eval: false
#what happens if it use the glm command?
#Doesn't work - obviously, the logit only predicts a dependent variable between 0 and 1, but our quota prices are more than 1

logit <- function(p) log( p / (1 - p))

glm(data = replic, 
    family = binomial(link = "logit"),
    b ~ live_priceGDP + quota_remaining_BOQ + fraction_remaining_BOQ + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter) %>% 
  summary()
```

# Model - With Climate Change

## Import Ecodata

```{r}
#| eval: false
#Command for installing ecodata if you don't have it yet
library(devtools)
remotes::install_github("noaa-edab/ecodata",build_vignettes=TRUE)
```

```{r}
library(ecodata)
```

## Clean Ecodata

We want to bring in indicators of climate change. Below are data sets of *ecodata* that are of interest:

```{r}
#For bottom sea temperature
#Data is only annually so need to fix
bottom_temp <- ecodata::bottom_temp

#Harmful algae blooms
#Data is only annually so need to fix
habs <- ecodata::habs

#Heatwave data
#Data is only annuall so need to fix
heatwave <- ecodata::heatwave

#Sea surface temerpature anomoly - not as great as bottom temp anomoly but still good
sst <- ecodata::seasonal_oisst_anom

#Storminess
#Data is only annually so need to fix
storms <- ecodata::storminess
```

Four of the five data sets are only reported annually. So, we should make them quarterly so that we can merge them with our `replic` data set.

\--

First thing we actually want to do is make the tables "pivot wider".

For `bottom_temp`: Also, the GOM and GB (George's Bank) observations are different. Thus, when merging with the `replic` data set, we will have to account for that difference. Shouldn't be too hard though - could always use a mutate(EPU=case_when()) and merge on the EPU.

For `heatwave`: Tracks both the intensity and duration of both the Sea-Surface-Temp (SST) and Bottom_Surface_Temp (BST). The variables `duration-BottomDetrended` and `duration-SurfaceDetrended` should describe the duration of heatwaves in a year. Are they summed for the year though?

The detrended in the variable names corresponds to the authors taking away the trend of global warming from the data. Meaning any deviation from 0 in the data corresponds with extra heating or cooling that can be attriubted to region specific climate change. For more information refer to the link at the bottom of the help page for the `heatwave` data set.

For `sst`: The link to the documentation can be seen [here](https://noaa-edab.github.io/tech-doc/seasonal-sst-anomalies.html). Describes the exact coding of "spring", "summer", "fall", and "winter". And, I recode into quarters to make my life easier since they have exact overlap.

```{r}
#This cell reformats the ecodata data sets into more usable formats

#Widen the bottom_temp data set and resrtic for only observations we care about
bottom_temp <- bottom_temp %>% 
  #Restrict to only the time frame in question
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Exclude MAB and SS
  filter(EPU != "MAB" & EPU != "SS") %>% 
  #Pivot the table wider - take observations from the same year and make them into columns instead of redundant rows to make mergering easier later
  pivot_wider(names_from = Var, values_from = Value)

habs <- habs %>% 
  #Restrict area to only GOM in aggregate
  filter(Var == "Gulf_of_Maine_All") %>% 
  #Only include years we care about
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Rename the Value to the variable it's observing
  rename(Algae = Value) %>% 
  #Select only the columns needed for merging
  dplyr::select(!c(Source, Var))

heatwave <- heatwave %>% 
  #Restict area to George's Bank (GB) and Gulf of Maine (GOM)
  filter(EPU == "GB" | EPU == "GOM") %>% 
  #Restrict time frame to 2010-2019
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Keep maximum intensity and duration of SS and BS heatwaves
  filter(Var != "cumulative intensity-SurfaceDetrended" & Var != "cumulative intensity-BottomDetrended") %>% 
  #Pivot this wider so that we only have 1 row for each observation in both GB and GOM
  #Deselect Units because it fucks with the pivot wider function
  dplyr::select(!Units) %>% 
  pivot_wider(names_from = Var, values_from = Value)

#SST is an anomoly measure, similar to the heatwave data set - Seems redundant due to the heatwave dataset. Heatwave seems much more complete and has more robust methods
sst <- sst %>% 
  #Select years 2010-2019
  filter(Time >= 2010 & Time <= 2019) %>% 
  #Recode the time series in terms of quarters instead of seasons to make merge easier later
  mutate(q = case_when(Var == "Winter" ~ "Q1",
                       Var == "Spring" ~ "Q2",
                       Var == "Summer" ~ "Q3",
                       Var == "Fall" ~ "Q4"),
         .after = Var) %>% 
  #Drop the Var colum since it's unnecessary
  dplyr::select(!Var) %>% 
  #Filter for only area of interest
  filter(EPU != "MAB")

#Probably need to come back and make this wider, but this works for now
storms <- storms %>% 
  #Select only years we care about
  filter(Year >= 2010 & Year <= 2019) %>% 
  #Consider only areas of interest
  filter(EPU == "GOM" | EPU == "GB") %>% 
  #Group by the year and general region to get a better sense of cumulative storms for this region
  group_by(Year, Var) %>% 
  summarize(mean_events = mean(Value),
            untis = units,
            EPU = EPU) %>% 
  #This data represents days in the year that a "storm" was recorded from a wind and wave      perspective
  #Make the data set tidy where each observation (year) is a row
  pivot_wider(names_from = Var, values_from = mean_events) %>% 
  #Take the average of the GOM observations and just use the GB for their respective rows
  mutate(Gale_Wind = case_when(EPU == "GOM" ~ mean(c(`Eastern Gulf of Maine_GaleWind`, `Western Gulf of Maine_GaleWind`), na.rm = T),
                               EPU == "GB" ~ `Georges Bank_GaleWind`), .after = EPU) %>% 
  #Same process as above mutate but for WaveHeight instead
  mutate(Wave_Height = case_when(EPU == "GOM" ~ mean(c(`Eastern Gulf of Maine_WaveHeight`, `Western Gulf of Maine_WaveHeight`), na.rm = T),
                               EPU == "GB" ~ `Georges Bank_WaveHeight`), .after = Gale_Wind) %>% 
  #Select only the columnds that matter for merging
  dplyr::select(Year, EPU, Gale_Wind, Wave_Height) %>% 
  #Rename the Year column to Time to make merge easier
  rename(Time = Year)
```

Now we can start merging the yearly data sets together

```{r}
#Join the data sets together
temp1 <- bottom_temp %>%
  #Join the heatwave and bottom temp data sets together
  right_join(heatwave, by = join_by(Time, EPU)) %>%
  #Select the columns that will be most helpful
  dplyr::select(Time, EPU, `bottom temp anomaly in situ`, `sst anomaly in situ`, `duration-SurfaceDetrended`, `duration-BottomDetrended`) %>% 
  #Join again with storms data
  right_join(storms, by = join_by(Time, EPU)) %>% 
  #Join again with algae bloom data - Don't have data on GB so might need to come back and alter that for regression
  left_join(habs, by = join_by(Time, EPU)) %>% 
  #Repeat each thing for 4 quarters
  slice(rep(1:n(), each = 4)) %>% 
  mutate(q = rep(c("Q1","Q2","Q3","Q4"), times=20), .after = Time) %>% 
  #Rename q to quarter and turn into factor
  rename(quarter = q) %>% 
  mutate(quarter = as.factor(quarter))
```

```{r}
#Rewrite the EPU's so we can merge data sets
replic <- replic %>% 
  mutate(EPU = case_when(stockarea == "CCGOM" ~ "GOM",
                         stockarea == "GBE" ~ "GB",
                         stockarea == "GBW" ~ "GB",
                         stockarea == "GB" ~ "GB",
                         stockarea == "GOM" ~ "GOM",
                         #Maddy says that Plaice is normally caught in GB
                         stockarea == "Unit" ~ "GB",
                         #SNEMA is closer to GOM than GB so we count it as such
                         stockarea == "SNEMA" ~ "GOM"), .after = stockcode)
```

```{r}
#This chunk appears to join everything together correctly

#Create the merged price and climate data set called "replic2"
replic2 <- replic %>% 
  right_join(temp1, join_by("fishing_year"=="Time", EPU, quarter))
```

## Results

I believe that including the climate change indicators in both hurdles makes the most sense. Since I predicted that climate change will generally affect quota prices this means we should see the climate change affecting both the probability of the quota being greater than 0 (hurdle 1) which implies the quota should be trading at a higher price (hurdle 2). Thus, we should capture both effects by including climate change in both stages of the hurdle model. An additional bonus to the Cragg model compared to the Tobit, including both means that their respective coefficients are independent. Meaning that we could see things like one being positive and one being negative thanks to the Cragg model.

Another shortcoming of Lee and Demarest could be that they didn't count for different stocks as dummy variables in their model. Possibly including this could lead to some more accurate models since not only do stocks have vastly different trading prices from factors such as demand, choke, etc. But in addition, different stocks are impacted differently by climate change. This could in turn mean that there is an interaction effect between the climate change indicators and the stocks.

Some of the variables didn't work in the following models so if the above rational doesn't appear to translate to the model, that's why.

### OLS

The easiest model of them all, OLS, can be run first to see what happens when including climate change data into the models.

```{r}
lm(data = replic2, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended`+ Gale_Wind + Wave_Height + quarter) %>% 
  summary()
```

```{r}
#Examine with lots of controls on things such as species, year, EPU, as well as climate

#Cannot include algae in the model
lm(data = replic2, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Gale_Wind + Wave_Height + fishing_year + quarter + EPU + stock_id) %>% 
  summary()
```

In interesting note is that including the species as a dummy variable almost doubles our $R^2$. Seems like it would be a no brainier to include in the model. I wonder why Lee and Demarest have decided not to include this in their model.

As discovered from [here](https://www.andrewheiss.com/blog/2022/05/09/hurdle-lognormal-gaussian-brms/#hurdle-lognormal-model-with-fancy-multilevel-things) and above, logging the OLS will probably create a better fitting OLS. So, let's consider log-log, linear-log, and log-linear below. However, when trying to directly take the log of any variable in `replic2` we get an error because `log(0)=-Inf` which causes a problem for the `lm()`. To fix this, one could just add a constant. `log(1)` is an obvious choice since it equals 0. But, at that point we have to seriously consider why we are doing it and what is the point. Honestly, there doesn't seem to be much of one. Since we know that OLS wouldn't fit well anyway and [this article](https://aosmith.rbind.io/2018/09/19/the-log-0-problem/) recommends that you should just use a Tobit model if the 0's in your data are censored. And our hurdle model is a fancy Tobit model so I honestly believe that we should just scrap the log-OLS idea and move on.

### Hurdle

In this section we recreate the hurdles made from above. I have not idea which ones will fit better so we'll recreate all of them and see which ones turn out the best.

```{r}
#Surface duration causes problems - "Hessian is singular" but only when using it in the second part of the hurdle
#Gale Wind also causes problems in both parts of the model
churdle.1a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Wave_Height | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + Wave_Height, 
                 data = replic2,
                 h2 = TRUE, dist = "n", method = "bhhh") 

churdle.1b <- update(churdle.1a, dist = "ln")
```

```{r}
modelsummary(models = list("c1a" = churdle.1a, "c1b" = churdle.1b),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

Current understanding of model outputs as of 7/19.

### Arguemetns Explained

-   `c1a` - In this model we are, obviously, using a two stage hurdle model represented by the `h2 = TRUE` argument. We are using are assuming that the errors(?) follow a normal distribution which is notated by the `dist = "n"` argument (for further reference check [here](https://christophm.github.io/interpretable-ml-book/extend-lm.html)). We use the "Berndt-Hall-Hall-Hausman" method of maximization represented by the `method = "bhhh"`.

    -   The argument `method = "bhhh"` is not on the `mhurlde` package vignette but on the `maxLik` one. So, to see what it means just type `?maxLik` or `help(maxLik)` into the console.

-   `c1b` -

### Coefficients Explained

While interpreting individal coefficients is difficult I think that since we are using the log normal argument for `c1b` (represented by the `dist="ln"`) we can understand that interpreting the second hurdle model's coefficients is the same as interpreting a log-log OLS model. A similar logic follows for `c1a` only since the `dist` argument changes to `"n"` this means that we can interpret the coefficients as a OLS model.

A follow up question though, does this mean that I could include fixed effects in the second hurdle and make it a fixed effects model? -\>

DEFINETLY SHOULD INCLUDE AT LEAST A TIME SERIES CONTROL SUCH AS YEAR IN SECOND HURDLE. Since the observations are not independent - ie. the quota prices from last year a definitely related to the quota prices this year which is one of the literal textbook examples of the assumptions we make about errors.

### Goodness of Fit Explained

GoF -\> Goodness of fit

It should be noted that we actually want to check the goodness of fits with the other functions from the `mhurdle` package such as the `vuongtest()`. But, there are some good outputs just using the `modelsummary` package.

-   `c1a` - In this model (and all models in `mhurdle` as far as I can tell) the `Log.Lik` is an import from the `maxLik` package and represents the log-likelihood of both models together. NOT the zero or count/pos/positive model. This should be reinforced when considering the other GoF measures. The `R2.zero` represents an $R^2$ value for the zero model. In our case, the first hurdle model. The `R2.pos` represents an $R^2$ value for the count (positive) model. In our case, the second hurdle model.

    -   In context for model `c1a` we find that the `Log.Lik` is about -873. Which you can concert to mean something but for us it doesn't mean very much. It is a great baseline for us though, in that we now have a very quick and dirty way that we can check this model to others (namely `c1b`). The `R2.zero` represents the $R^2$ value for the the first hurdle model, which would be about .4. However, my current understanding is that the first hurdle is a logistic (like) regression which would mean this is more like a $pseudo-R^2$ than an actual $R^2$. The `R2.pos` is the $R^2$ for the second hurdle which acts more like an OLS regression which we can tell from the `dist = "n"` argument which means we are predicting a normal distribution for the positive values. We can tell from the almost objectively low value of .18 that this is not doing a very good job at fitting the data.

-   `c1b` -

From these initial models we can see that our explanatory power has definitely increased as compared with the models from the no climate change data. We see significance on some of the climate coefficients from the first hurdle but don't see any in the second part of the hurdle. Which I think is very interesting. Below is a continuation of the models, with them becoming more complex.

```{r}
churdle.2a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Wave_Height | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + Wave_Height, data = replic2,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh", finalHessian = TRUE)

churdle.2b <- update(churdle.2a, start = coef(churdle.2a), robust = FALSE)
churdle.2c <- update(churdle.2a, h2 = TRUE)
churdle.2d <- update(churdle.2c, start = coef(churdle.2c), robust = FALSE)
```

```{r}
modelsummary(models = list("c2a" = churdle.2a, "c2b" = churdle.2b, "c2c" = churdle.2c, "c2d"= churdle.2d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

These are the more complex models. Still don't know how to interpret but it looks like the log-likelihood is lower for both so it would appear that these models fit worse.

```{r}
churdle.3a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + `duration-SurfaceDetrended` + Wave_Height | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter + `bottom temp anomaly in situ` + `sst anomaly in situ` + `duration-BottomDetrended` + Wave_Height, data = replic2,
                     h2=FALSE, corr=TRUE, method="bhhh", finalHessian=TRUE)
churdle.3b <- update(churdle.3a, start = coef(churdle.3a), robust = FALSE)
churdle.3c <- update(churdle.3a, h2=TRUE)
churdle.3d <- update(churdle.3c, start = coef(churdle.3c), robust = FALSE)
```

```{r}
modelsummary(models = list("c3a" = churdle.3a, "c3b" = churdle.3b, "c3c" = churdle.3c, "c3d"= churdle.3d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

These are the most complex models. Not sure how the differ from the others but they don't have much better log-likelihoods so I would hesitate to use them.

## Kanae Recommendations

### 90th Percentile Dummy Variables

We can take some of the climate change variables (namely SST and BST) that we use above and create dummy variables for them to see if they are in the 90th percentile. This should mean that in the years where we see extreme heat deciations that we see changes in the behavior of stocks. This should lead to this dummy variable having a statistically significant and positive impact on the quota price.

```{r}
#| eval: false
#Represents the 90th percentile of SST and BST anomolies
quantile(replic2$`sst anomaly in situ`, probs = .9)
quantile(replic2$`bottom temp anomaly in situ`, probs = .9)
```

```{r}
#Add in the 90th percentile dummy variable for SST and BST

replic2 <- replic2 %>% 
  #Create the ifelse command to test if the observation makes the 90th percentile cutoff
  mutate(SST90 = ifelse(`sst anomaly in situ` >= quantile(replic2$`sst anomaly in situ`, probs = .9), 1, 0),.after = `sst anomaly in situ`) %>% 
  #Repeat but for BST
  mutate(BST90 = ifelse(`bottom temp anomaly in situ` >= quantile(replic2$`bottom temp anomaly in situ`, probs = .9), 1, 0),.after = `bottom temp anomaly in situ`)
```

Now that we have added the dummy variable colums we can add it to the respective regressions.

To start, we can apply this to the OLS regression since that the is easiest to preform and interpret. We will postpone doing this with the hurdle model until we have a better understanding of it's output.

```{r}
lm(data = replic2, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + `bottom temp anomaly in situ` + BST90 + `sst anomaly in situ` + SST90 + `duration-BottomDetrended` + `duration-SurfaceDetrended`+ Gale_Wind + Wave_Height + quarter) %>% 
  summary()
```

A couple of interesting notes:

-   SST got dropped because of multi-colinearity. We may find better results if we make this a logarithmic to catch non-linearities

-   The BST90 is highly significant and it has quite a large coefficient relative to almost all other considered variables other than `proportion_observed`

-   BST90 is negative which is quite interesting. I would have expected it to be positive since heatwaves should create more scarcity by driving the fish away. However, this is just an assumption that I have about fish migratory patterns. It is also quite possible that the warmer water does not drive away the fish in the short term but makes them easier to catch. Hence, why the coefficent is negative.

-   There is not a significant jump in $R^2$ or $Adj$ $R^2$ for this model compared to the other one in the climate change section above. I wouldn't expect a huge jump but I did expect more than a negligible amount.

### Adjusted Assessment Years

We know that stock assessments are flawed but they are used to determine the TAC for stocks in a year. However in (Kerr at al., 2022) we know from [this table](https://academic.oup.com/view-large/377923733) that there are some years where the assessment was so flawed it had to go back and be revised(?). This means that going through each year by stock and assigning a dummy variable should allow us to see if these misallocations of stocks have an impact on quota prices. We can infer that significance implies it does because these changes in the stock assessment (are endogenous) are due to mostly climate driven model misspecifications and some over-fishing.

```{r}

```

# Further Directions

Obviously we need to figure out exactly what the differences are between these models at an in depth level as well as figure out if our hypothesis is correct. It's looking like it's not sometimes so we'll have to see. This is described in the list below

1.  Write this out an in much more coherent way for the audiences that need to see what I've been doing - Namely Kanae, Departmental Honors Comittee, GMRI presentation and staff
2.  Find out what the other goodness of fit measures are
3.  Find out what the coefficients mean - Both in interpretation and in significance
4.  Have the models output into htmlreg or texreg so that it's prettier - These won't render in a powerpoint so actually not much of a point to it beyond this document. But we should try and get them to look better in the `modelsummary` package. Or see if texreg has a png or some other kind of picture generator for these tables
5.  Model the number of trades per year using the same explanatory variables - However we probably want to use a GLM using a Poisson distribution or some other kind of distribution since it probably isn't Gaussian. This would give us some more insight into our prediction that climate change is driving an increase in quota trades which in turn increases their price because of transaction costs
6.  Building off of point 5, including number of trades in a given year as a predictor should be included in all models considered. Currently, I implicitly assume with this model that the process of climate change -\> more trades -\> higher quota prices occurs by only looking at the effect climate change has on quota prices. But it is also plausible that these models are describing that climate change affects scarcity which in turn drives prices. To better answer my research question, we need to include number of quota trades in a year as a predictor. Probably won't have time to tackle this before school but this would be a good place to head towards for the Departmental Honors project.
