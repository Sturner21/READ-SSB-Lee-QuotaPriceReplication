---
title: "Cragg"
format: 
  html:
    self-contained: true
    toc: true
    toc-location: right
execute: 
  warning: false
---

## Libraries

There are tons of warnings from packages masking and such. So, in the YAML I have suppressed warnings for rendering.

```{r}
#Load in most of the necessary packages for this project
library(tidyverse)
library(mhurdle)
#These just keep overwriting eachother so I'll load them in as I need them in this document
#library(AER)
#library(pscl)
#library(countreg)
#library(lmtest)
```

# Replicate Documentation

The documentation for the *mhurdle* package can be found [here](https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf). We will be replicating Section 5, **Software Rationale**. In this replication we will be trying to fit the hurdle model to how much households spend on the "fees and admission" good. This should have a lot of 0 observations because some households will not spend any money on them at all, others will infrequently, and others still will spend a good portion of their income on it. This should be a good introduction package, and this data should represent the quota prices that I ultimately want to test well.

```{r}
#Grab the data that the documentation uses
data("Interview", package = "mhurdle")
```

The covariates of this data set are:

1.  Income - Annual net income by consumption unit
2.  SMSA - Does the household live in SMSA (yes or no)
3.  Age - The age of the reference person of the household
4.  Educ - The number of years of education of the reference person of the household
5.  Sex - The sex of the reference person of the household (male or female)
6.  Size - The number of persons in the household
7.  Month - The month of the interview (Between 1 to 12)

The analysis uses the households spending on the "fees and admission" good which is called *shows* in the data set.

```{r}
#What is the mean value, how many observations are 0, and what is the average for non-0 observations
round(c(mean = mean(Interview$shows),
        "% of 0" = mean(Interview$shows == 0),
        "mean pos" = mean(Interview$shows) / mean(Interview$shows > 0)),
      2)
```

These are good summary statistics, but if you actually want to see some of the data from *Interview*, here:

```{r}
#Just check out some of the data set
head(Interview %>% 
  #select() is masked by some other pakcage so I have to explicitly call it from dplyr to use   #it
  dplyr::select(income, smsa, age, educ, sex, size, month, shows))
```

To illustrate the use of *mhurdle* we will start with a single equation tobit model using both the normal and log-normal specification

```{r}
#Not sure this ever appears again. Certainly not in the below table so not sure what the point of it is
N010I <- mhurdle(shows ~ 0 | linc + smsa + age + educ + size, 
                 data = Interview,
                 h2 = TRUE, dist = "n", method = "bhhh") 

L010I <- update(N010I, dist = "ln")
```

The naming convention for the models in this documentation is as follows:

-   Each model is 5 digits long

-   The first capital letter denotes the distribution used - N for normal and L for log-normal

-   The second, third, and fourth are binary representing if the first, second, or third hurdle has been used (1 if used, 0 if not used)

-   The fifth digit is either D or I depending on if the model is dependent (D) or independent (I)

We should consider two other hurdle models, namely selection process and infrequency of purchase.

```{r}
#Creates a lot of models, some of which are displayed in the table below
L100D <- mhurdle(shows ~ smsa + age + educ + size | linc, data = Interview,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh",
finalHessian = TRUE)
L100D2 <- update(L100D, start = coef(L100D), robust = FALSE)
L110D <- update(L100D, h2 = TRUE)
L110D2 <- update(L110D, start = coef(L110D), robust = FALSE)


L001D <- mhurdle(shows ~ 0 | linc | smsa + age +
educ + size, data = Interview,
                   h2 = FALSE, corr = TRUE, method = "bhhh",
                 finalHessian = TRUE)
L001D2 <- update(L001D, start = coef(L001D), robust = FALSE) 
L011D <- update(L001D, h2 = TRUE)
L011D2 <- update(L011D, start = coef(L011D), robust = FALSE)
```

```{r}
#Makes some more of the models that appear in the table below
L101D <- mhurdle(shows ~ educ + size | linc |
smsa + age, data = Interview,
                    h2 = FALSE, method = "bhhh", corr = TRUE,
                 finalHessian = TRUE)
L101D2 <- update(L101D, start = coef(L101D), robust = FALSE) 
L111D <- update(L101D, h2 = TRUE)
L111D2 <- update(L111D, start = coef(L111D), robust = FALSE) 
L111I <- update(L111D, corr = FALSE)
L111I2 <- update(L111I, start = coef(L111I), robust = FALSE)
```

The results are shown in the table below

```{r}
#Sets up the list of the models that are considered in the table below. Plus, it outlines how to seperate the table by the different models which will be helpful in my final rendering
models <- list(L110D = L110D, L011D = L011D, 
               L101D = L101D, L111D = L111D, L111I = L111I)
coefs <- unique(Reduce("c", lapply(models, function(x) names(coef(x)))))
coefs1 <- grep("h1", coefs)
coefs2 <- grep("h2", coefs)
coefs3 <- grep("h3", coefs)
coefso <- (1:length(coefs))[-c (coefs1, coefs2, coefs3)]
custcoefs <- coefs
custcoefs[coefso] <- c("$\\sigma$", "$\\rho_{12}$", "$\\alpha$", "$\\rho_{23}$", "$\\rho_{13}$")
coefs.h <- grep("h.\\.", coefs)
#custcoefs[coefs.h] <- substring(custcoefs[coefs.h], 4, 1E4)
groups <- list("**Hurdle 1**" = 1:length(coefs1),
               "**Hurlde 2**" = (length(coefs1)+1):(length(coefs1) + length(coefs2)),
               "**Hurdle 3**"= (length(coefs1)+length(coefs2)+1):(length(coefs1) + length(coefs2) + length(coefs3)),
               "**Others**" = (length(coefs1) + length(coefs2) + length(coefs3) + 1) : (length(coefs1) + length(coefs2) + length(coefs3) + length(coefso)))
#coeflabels <- gsub("h[1-3]\\.(\\w*)", "\\1", coefs)
```

```{r}
#| eval: true
#| results: asis

#In the original document this is a texreg because it's a PDF. However, I am rendering a HTML document so I've made the slight alteration of using the htmlreg command

texreg::htmlreg(models, reorder.coef = c(coefs1, coefs2, coefs3, coefso), 
       custom.coef.names = custcoefs,#[c(coefs1, coefs2, coefs3, coefso)],
       caption = "Estimation of cencored models for the fees and admissions good",
       label = "tab:estimations",
       groups = groups
       )
```

For my work, it looks like I'll want to use the code that creates the the model **L110D** since that creates the two part hurdle.

Below are some significance tests that the paper runs. I do not fully understand this so circling back may be helpful. However, to start I just want to replicate Lee & Demarest's findings so all these tests aren't necessary. Only once I start considering my own will they be helpful.

```{r}
library(lmtest)
lrtest(L111D, L111I)
```

```{r}
vuongtest(L111D, L111I, type = "nested")
```

```{r}
ndvuongtest(L110D, L011D)
```

This concludes the replication of the documentation.

# Clean main Data

We need to get the data from "quarterly_ols_coefs_from_R_2022_03_04.dta", "spatial_lags_2022_03_04.dta", and "Tspatial_lags_2022_03_04.dta" into a singular, tidy, data frame so that we can start replicating the results. Below is the code of how I combined and cleaned the data.

```{r}
#We need to load all the different data sets in
library(haven)

quarterly_fish_prices <- read_dta("quarterly_ols_coefs_from_R_2022_03_04.dta")
Tspatial_lags <- read_dta("Tspatial_lags_2022_03_04.dta")
```

```{r}
#Select the variables of interest from each data set

#Select variables from quarterly fish prices
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  dplyr::select(fishing_year, q_fy, b, dateq, stockcode, stock_id, stock, nespp3, stockarea, spstock2, quota_remaining_BOQ, fraction_remaining_BOQ, proportion_observed, live_priceGDP)

#Select variables of interest from each data set
Tspatial_lags <-  Tspatial_lags %>% 
  dplyr::select(fishing_year, dateq, stockcode, WTswt_quota_remaining_BOQ, WTDswt_quota_remaining_BOQ)
```

There are a different amount of observations in the two data sets, so lets make them the same length so that the join is 1:1.

```{r}
Tspatial_lags
```

```{r}
#Further select down for the variables that are shared between quarterly and Tspatial data sets

#Want to get this to the 672 observations of "Tspatial_lags"
quarterly_fish_prices <-  quarterly_fish_prices %>% 
  #Helps get rid of some observations - Gets to 884 rows
  dplyr::filter(stockcode != 1818 & stockcode != 9999) %>% 
  #Gets down to 680 rows
  dplyr::filter(fishing_year >= 2010 & fishing_year <= 2019)
```

Let's try to combine the data sets now and see which ones have missing values to pinpoint the non-overlapping observations.

```{r}
replic <-  dplyr::right_join(Tspatial_lags, quarterly_fish_prices, by = c("fishing_year", "dateq", "stockcode")) %>% 
  #This filters out the non-overlapping parts of our dataset
  dplyr::filter(!is.na(WTswt_quota_remaining_BOQ))

#Even though there are som missing values for "b", there shouldn't be an issue running regressions on it
```

May not need this below chunk. Technically, there is quarter already coded into the data set, but I'm not sure how much I trust it.

```{r}
#Add in factor variable quarter

replic <-  replic %>%
  #Rearrange 'replic' so that we have each stock in chronological order
  dplyr::arrange(stockcode) %>% 
  #Add in a quarter variable
  dplyr::mutate(quarter=rep(c("Q1","Q2","Q3","Q4"), times=168), .after = fishing_year) %>% 
  #Make the quarter variable a factor variable so regression recognizes it as a dummy variable
  dplyr::mutate(quarter = as.factor(quarter))
```

```{r}
head(replic)
```

Finally, we need to adapt the quota prices to the assumptions made by Lee & Demarest, we need to make all the NA's and negative values into 0's.

```{r}
replic <- replic %>%
  #Stage 1 is to make any negative values into 0's
  dplyr::mutate(b = case_when(b < 0 ~ 0,
                              b >=0 ~ b)) %>% 
  #Stage 2 is to make any NA's into 0's
  dplyr::mutate(b = replace_na(b, 0))
```

# Model - No Climate Change

## mhurdle package method

```{r}
#For quick referenceing of models because "summary()" doesn't work with mhurdle package
library(modelsummary)
```

Let's try to make a hurdle model.

```{r}
hurdle.1a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, 
                 data = replic,
                 h2 = TRUE, dist = "n", method = "bhhh") 

hurdle.1b <- update(hurdle.1a, dist = "ln")
```

```{r}
modelsummary(models = list("1a" = hurdle.1a, "1b" = hurdle.1b),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

This verbatim makes the models that we started with in the documentation for the hurdle model. The model summary lets us easily take a look at them and their significance. Let's try to make some more complicated ones.

```{r}
hurdle.2a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, data = replic,
                 h2 = FALSE, dist = "ln", corr = TRUE, method = "bhhh", finalHessian = TRUE)

hurdle.2b <- update(hurdle.2a, start = coef(hurdle.2a), robust = FALSE)
hurdle.2c <- update(hurdle.2a, h2 = TRUE)
hurdle.2d <- update(hurdle.2c, start = coef(hurdle.2c), robust = FALSE)
```

```{r}
modelsummary(models = list("2a" = hurdle.2a, "2b" = hurdle.2b, "2c" = hurdle.2c, "2d"= hurdle.2d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

The more complicated models have now been replicated. Let's try the most complex finally and see what they look like.

```{r}
hurdle.3a <- mhurdle(b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter | live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter, data = replic,
                     h2=FALSE, corr=TRUE, method="bhhh", finalHessian=TRUE)
hurdle.3b <- update(hurdle.3a, start = coef(hurdle.3a), robust = FALSE)
hurdle.3c <- update(hurdle.3a, h2=TRUE)
hurdle.3d <- update(hurdle.3c, start = coef(hurdle.3c), robust = FALSE)
```

```{r}
modelsummary(models = list("3a" = hurdle.3a, "3b" = hurdle.3b, "3c" = hurdle.3c, "3d"= hurdle.3d),
             stars = c('*' = .1, '**' = .05, '***' = 0.01))
```

## UVA method

I will need to use different packages than *countreg* since I need distributions similar to Poisson but that are continuous. Internet says the *GAMLSS* has such distributions.

```{r}
library(AER)
library(pscl)
library(countreg)
```

```{r}
mod1 <- glm(data = replic, family = "gaussian",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) 

mod1 %>% 
  summary()
```

Quota prices that are negative are probably just modelling error since econ theory (and common sense) would suggest that agents wouldn't pay to have quota taken off their hands. So, for the sake of argument, let's eliminate them from consideration so we can try and fit a poisson distribution.

```{r}
#| warning: false

#Suppres the ungodly amount of warnings from the errors of trying the poisson

glm(data = replic, family = "poisson",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) %>% 
  summary()
```

```{r}
#Use the quasipoisson to correct for the prices which are not integers but it also means that I have no AIC
#Not the worst thing in the world since I had AIC=inf when using just "poisson"
mod2 <- glm(data = replic, family = "quasipoisson",
    b ~ quota_remaining_BOQ + fraction_remaining_BOQ + proportion_observed + quarter + live_priceGDP + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ) 

mod2 %>% 
  summary()
```

Hurdle model

Could just multiply by 100000 and round

Can I just make a two stage model - part one being logistic and part two being OLS (or something similar)

```{r}
#| eval: false
hurdle(data = replic, b ~ .) %>% 
  summary()
```

## OLS method

Trying to replicate the OLS model.

```{r}
#Seeing if we can replicate the 'OLS' column of Table 5 from Lee & Demarest

lm(data = replic, b ~ live_priceGDP + quota_remaining_BOQ + proportion_observed + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter) %>% 
  summary()
```

```{r}
#| eval: false
#what happens if it use the glm command?
#Doesn't work - obviously, the logit only predicts a dependent variable between 0 and 1, but our quota prices are more than 1

logit <- function(p) log( p / (1 - p))

glm(data = replic, 
    family = binomial(link = "logit"),
    b ~ live_priceGDP + quota_remaining_BOQ + fraction_remaining_BOQ + WTswt_quota_remaining_BOQ + WTDswt_quota_remaining_BOQ + quarter) %>% 
  summary()
```

# Model - With Climate Change

Need to import the climate change data from NOAA's *ecodata* package.
